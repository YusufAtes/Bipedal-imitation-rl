{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ezc3d import c3d\n",
    "from scipy.signal import butter, filtfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "def lowpass_filter_data(data, cutoff=4, fs=100, order=4):\n",
    "    \"\"\"\n",
    "    Apply a Butterworth lowpass filter to the data along each column.\n",
    "\n",
    "    Parameters:\n",
    "      data   : 2D NumPy array of shape (n, 5)\n",
    "      cutoff : Cutoff frequency in Hz (default is 4 Hz)\n",
    "      fs     : Original sampling frequency (default is 100 Hz)\n",
    "      order  : Order of the Butterworth filter (default is 4)\n",
    "    \n",
    "    Returns:\n",
    "      Filtered data as a NumPy array with the same shape as the input.\n",
    "    \"\"\"\n",
    "    nyq = 0.5 * fs                  # Nyquist Frequency\n",
    "    normal_cutoff = cutoff / nyq    # Normalized cutoff frequency\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    # Use filtfilt to avoid phase shift.\n",
    "    filtered_data = filtfilt(b, a, data, axis=0)\n",
    "    return filtered_data\n",
    "\n",
    "def downsample(data, orig_rate=100, new_rate=10):\n",
    "    \"\"\"\n",
    "    Downsample the data from orig_rate to new_rate by taking every (orig_rate/new_rate)-th sample.\n",
    "    \n",
    "    Parameters:\n",
    "      data      : 2D NumPy array of shape (n, 5)\n",
    "      orig_rate : Original sampling frequency (Hz)\n",
    "      new_rate  : Desired sampling frequency (Hz)\n",
    "    \n",
    "    Returns:\n",
    "      Downsampled data.\n",
    "    \"\"\"\n",
    "    factor = orig_rate // new_rate\n",
    "    return data[::factor]\n",
    "\n",
    "def find_near_zero_blocks(data, tol=1e-2, min_gap=30):\n",
    "    \"\"\"\n",
    "    Identify the start indices of contiguous blocks where all 5 columns are near zero.\n",
    "    A new near-zero block is only accepted if it is at least 'min_gap' samples after the previous one.\n",
    "\n",
    "    Parameters:\n",
    "      data    : 2D NumPy array of shape (n, 5)\n",
    "      tol     : Tolerance to consider a value as zero (default is 1e-2)\n",
    "      min_gap : Minimum number of samples between consecutive near-zero blocks (default is 30)\n",
    "    \n",
    "    Returns:\n",
    "      A NumPy array of filtered start indices for near-zero blocks.\n",
    "    \"\"\"\n",
    "    # Create a boolean mask that is True when all 5 columns are nearly zero.\n",
    "    near_zero = np.all(np.abs(data[:,:4]) < tol, axis=0)\n",
    "    \n",
    "    # Compute differences to identify transitions from non-zero to near-zero.\n",
    "    diff = np.diff(near_zero.astype(int))\n",
    "    starts = np.where(near_zero == True)[0]   # +1 to point to the first True value in the block\n",
    "    \n",
    "    # If the very first sample is near zero, include index 0.\n",
    "    if near_zero[0]:\n",
    "        starts = np.insert(starts, 0, 0)\n",
    "    \n",
    "    # Filter out indices that are too close together.\n",
    "    filtered_starts = []\n",
    "    if len(starts) > 0:\n",
    "        filtered_starts.append(starts[0])\n",
    "        for idx in starts[1:]:\n",
    "            if idx - filtered_starts[-1] >= min_gap:\n",
    "                filtered_starts.append(idx)\n",
    "    \n",
    "    return np.array(filtered_starts)\n",
    "\n",
    "def find_cycle_boundaries(data, tol=1e-2, min_gap=30):\n",
    "    \"\"\"\n",
    "    Identify cycle boundaries in the periodic data. Here, a cycle is assumed to start at the first \n",
    "    near-zero block and end at the start of the next near-zero block that is at least 'min_gap' samples later.\n",
    "    \n",
    "    Parameters:\n",
    "      data    : 2D NumPy array (downsampled) of shape (n, 5)\n",
    "      tol     : Tolerance to consider a value as zero (default is 1e-2)\n",
    "      min_gap : Minimum number of samples between consecutive near-zero blocks (default is 30)\n",
    "    \n",
    "    Returns:\n",
    "      A tuple (cycle_start, cycle_stop) indicating the start and stop indices of one cycle.\n",
    "    \"\"\"\n",
    "    near_zero_starts = find_near_zero_blocks(data, tol, min_gap)\n",
    "    \n",
    "    if len(near_zero_starts) < 2:\n",
    "        raise ValueError(\"Not enough near-zero regions found to determine a cycle!\")\n",
    "    \n",
    "    cycle_start = near_zero_starts[0]\n",
    "    cycle_stop = near_zero_starts[1]\n",
    "    return cycle_start, cycle_stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lowpass_filter(data, cutoff=5, fs=100, order=4):\n",
    "#     nyquist = 0.5 * fs\n",
    "#     normal_cutoff = cutoff / nyquist\n",
    "#     b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "#     y = filtfilt(b, a, data, axis=0)\n",
    "#     return y\n",
    "\n",
    "def extract_gait_cycle(c, gait_duration=3,mode='fft'):\n",
    "\n",
    "    times = c[\"parameters\"][\"EVENT\"][\"TIMES\"]['value']\n",
    "    contexts = c[\"parameters\"][\"EVENT\"][\"CONTEXTS\"]['value']\n",
    "    labels = c[\"parameters\"][\"EVENT\"][\"LABELS\"]['value']\n",
    "    gait_points = []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        if (labels[i] == \"Foot Strike1\") and (contexts[i] == \"Right\"):\n",
    "            gait_start = int(times[1,i]*100)\n",
    "        if (labels[i] == \"Foot Strike2\") and (contexts[i] == \"Right\"):\n",
    "            gait_end = int(times[1,i]*100+1)\n",
    "\n",
    "    trial_data = c['data']['points']\n",
    "    labels = c['parameters']['POINT']['LABELS']['value']\n",
    "\n",
    "    #[15,4,16,5,20,9]\n",
    "    r_ftc = labels.index(\"R_FTC\") if \"R_FTC\" in labels else RuntimeError(\"R_FTC not found\")\n",
    "    l_ftc = labels.index(\"L_FTC\") if \"L_FTC\" in labels else RuntimeError(\"L_FTC not found\")\n",
    "    r_fle = labels.index(\"R_FLE\") if \"R_FLE\" in labels else RuntimeError(\"R_FLE not found\")\n",
    "    l_fle = labels.index(\"L_FLE\") if \"L_FLE\" in labels else RuntimeError(\"L_FLE not found\")\n",
    "    r_fal = labels.index(\"R_FAL\") if \"R_FAL\" in labels else RuntimeError(\"R_FAL not found\")\n",
    "    l_fal = labels.index(\"L_FAL\") if \"L_FAL\" in labels else RuntimeError(\"L_FAL not found\")\n",
    "    r_fcc = labels.index(\"R_FCC\") if \"R_FCC\" in labels else RuntimeError(\"R_FCC not found\")\n",
    "    r_fm1 = labels.index(\"R_FM1\") if \"R_FM1\" in labels else RuntimeError(\"R_FM1 not found\")\n",
    "    l_fcc = labels.index(\"L_FCC\") if \"L_FCC\" in labels else RuntimeError(\"L_FCC not found\")\n",
    "    l_fm1 = labels.index(\"L_FM1\") if \"L_FM1\" in labels else RuntimeError(\"L_FM1 not found\")\n",
    "\n",
    "    sjn = labels.index(\"SJN\") if \"SJN\" in labels else RuntimeError(\"SJN not found\")\n",
    "    sxs = labels.index(\"SXS\") if \"SXS\" in labels else RuntimeError(\"SXS not found\")\n",
    "\n",
    "    r_leglength_ind = c['parameters']['SUBJECT']['LABELS']['value'].index(\"R_legLength\") if \"R_legLength\" in c['parameters']['SUBJECT']['LABELS']['value'] else RuntimeError(\"R_LEGLENGTH not found\")\n",
    "    l_leglength_ind = c['parameters']['SUBJECT']['LABELS']['value'].index(\"L_legLength\") if \"L_legLength\" in c['parameters']['SUBJECT']['LABELS']['value'] else RuntimeError(\"L_LEGLENGTH not found\")\n",
    "    weight = c['parameters']['SUBJECT']['LABELS']['value'].index(\"weight\") if \"weight\" in c['parameters']['SUBJECT']['LABELS']['value'] else RuntimeError(\"Weight not found\")\n",
    "    height = c['parameters']['SUBJECT']['LABELS']['value'].index(\"height\") if \"height\" in c['parameters']['SUBJECT']['LABELS']['value'] else RuntimeError(\"Height not found\")\n",
    "    r_leglength = c['parameters']['SUBJECT']['VALUES']['value'][r_leglength_ind]\n",
    "    l_leglength = c['parameters']['SUBJECT']['VALUES']['value'][l_leglength_ind]\n",
    "    weight = c['parameters']['SUBJECT']['VALUES']['value'][weight]\n",
    "    height = c['parameters']['SUBJECT']['VALUES']['value'][height]\n",
    "\n",
    "    muscle_index = [r_ftc,l_ftc,r_fle,l_fle,r_fal,l_fal,sxs,sjn,r_fcc,r_fm1,l_fcc,l_fm1]\n",
    "    joint_data = trial_data[:,muscle_index,:]\n",
    "    joint_states = np.zeros((joint_data.shape[2],6))\n",
    "\n",
    "    data_len = joint_data.shape[2]\n",
    "\n",
    "    first_pos = trial_data[0,sxs,0]\n",
    "    last_pos = trial_data[0,sxs,-1]\n",
    "\n",
    "    if last_pos > first_pos:\n",
    "        forward_walk = True\n",
    "    else:\n",
    "        forward_walk = False\n",
    "\n",
    "    speed_muscle = trial_data[:,sxs,:]\n",
    "    speed = (np.abs(speed_muscle[0,-1]-speed_muscle[0,0])/(len(speed_muscle[0,:])))/10\n",
    "    #data_point[:,-1] = speed\n",
    "\n",
    "    if forward_walk:\n",
    "        for i in range(data_len):\n",
    "            joint_states[i,0] = np.arctan2((joint_data[0,2,i] - joint_data[0,0,i])   , (-joint_data[2,2,i] + joint_data[2,0,i])) \n",
    "            joint_states[i,1] = np.arctan2((joint_data[0,4,i] - joint_data[0,2,i])   , (-joint_data[2,4,i] + joint_data[2,2,i]))  - joint_states[i,0]\n",
    "            joint_states[i,2] = np.arctan2((joint_data[2,9,i] - joint_data[2,8,i])   , (joint_data[0,9,i] - joint_data[0,8,i]))   - joint_states[i,0] - joint_states[i,1]\n",
    "            joint_states[i,3] = np.arctan2((joint_data[0,3,i] - joint_data[0,1,i])   , (-joint_data[2,3,i] + joint_data[2,1,i]))\n",
    "            joint_states[i,4] = np.arctan2((joint_data[0,5,i] - joint_data[0,3,i])   , (-joint_data[2,5,i] + joint_data[2,3,i]))  - joint_states[i,3]\n",
    "            joint_states[i,5] = np.arctan2((joint_data[2,11,i] - joint_data[2,10,i]) , (joint_data[0,11,i] - joint_data[0,10,i])) - joint_states[i,3] - joint_states[i,4]\n",
    "\n",
    "    else:\n",
    "        for i in range(data_len):\n",
    "            joint_states[i,0] = np.arctan2((-joint_data[0,2,i] + joint_data[0,0,i]) , (-joint_data[2,2,i] + joint_data[2,0,i])) \n",
    "            joint_states[i,1] = np.arctan2((-joint_data[0,4,i] + joint_data[0,2,i]) , (-joint_data[2,4,i] + joint_data[2,2,i]))     - joint_states[i,0]\n",
    "            joint_states[i,2] = np.arctan2((joint_data[2,9,i] - joint_data[2,8,i])   ,(-joint_data[0,9,i] + joint_data[0,8,i]))     - joint_states[i,0] - joint_states[i,1]\n",
    "            joint_states[i,3] = np.arctan2((-joint_data[0,3,i] + joint_data[0,1,i]) , (-joint_data[2,3,i] + joint_data[2,1,i]))\n",
    "            joint_states[i,4] = np.arctan2((-joint_data[0,5,i] + joint_data[0,3,i]) , (-joint_data[2,5,i] + joint_data[2,3,i]))     - joint_states[i,3]\n",
    "            joint_states[i,5] = np.arctan2((joint_data[2,11,i] - joint_data[2,10,i]) ,(-joint_data[0,11,i] + joint_data[0,10,i]))   - joint_states[i,3] - joint_states[i,4]\n",
    "\n",
    "\n",
    "    total_gait_sample = 32\n",
    "    joint_states = lowpass_filter_data(joint_states)\n",
    "    # gait_start, gait_end = find_cycle_boundaries(joint_states, tol=1e-1, min_gap=30)\n",
    "    if mode == 'fft':\n",
    "        joint_states = joint_states[gait_start:gait_end,:]\n",
    "        joint_states = downsample(joint_states, orig_rate=100, new_rate=10)\n",
    "    \n",
    "    if mode == 'time':\n",
    "        return joint_states\n",
    "    else:\n",
    "        \n",
    "        if joint_states.shape[0] > total_gait_sample:\n",
    "            print(f'gait cycle: {joint_states.shape[1]} longer than {gait_duration} seconds')\n",
    "                \n",
    "        while joint_states.shape[0] < total_gait_sample:\n",
    "            joint_states = np.concatenate((joint_states, joint_states),axis=0)  # Stack horizontally\n",
    "\n",
    "        joint_states = joint_states[:total_gait_sample,:]\n",
    "        # padded_joint_states = np.zeros((512, 4))\n",
    "        # padded_joint_states[46:46+joint_states.shape[0],:] = joint_states\n",
    "        \n",
    "        joint_states_rfft = np.fft.rfft(joint_states, n=32,axis=0)\n",
    "        real_fft = np.real(joint_states_rfft)\n",
    "        imag_fft = np.imag(joint_states_rfft)\n",
    "        joint_states_rfft = np.stack((real_fft, imag_fft), axis=2)\n",
    "        freq_values = np.fft.rfftfreq(32, 1/10)\n",
    "        encoder_vec = np.empty((3))   # init_pos + speed + r_leglength + l_leglength + ramp_angle = 0\n",
    "        # encoder_vec[0:4] = joint_states[:,0]\n",
    "        encoder_vec[0] = speed/3\n",
    "        encoder_vec[1] = r_leglength /1.5\n",
    "        encoder_vec[2] = l_leglength /1.5\n",
    "        # encoder_vec[3] = weight / 100  # 100 is the maximum weight in the dataset\n",
    "        # encoder_vec[4] = height / 2    # 1.91 m is the maximum height in the dataset\n",
    "\n",
    "        encoder_vec = encoder_vec[np.newaxis, :]\n",
    "        joint_states_rfft = joint_states_rfft[np.newaxis, :, :] #179 is the maximum value in the dataset\n",
    "        return joint_states_rfft, freq_values, encoder_vec, joint_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gather Time Series Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 50, 6)\n",
      "(6, 50, 6)\n",
      "(9, 50, 6)\n",
      "(15, 50, 6)\n",
      "(23, 50, 6)\n",
      "(26, 50, 6)\n",
      "(29, 50, 6)\n",
      "(35, 50, 6)\n",
      "(39, 50, 6)\n",
      "(43, 50, 6)\n",
      "(46, 50, 6)\n",
      "(49, 50, 6)\n",
      "(57, 50, 6)\n",
      "(60, 50, 6)\n",
      "(63, 50, 6)\n",
      "(70, 50, 6)\n",
      "(73, 50, 6)\n",
      "(76, 50, 6)\n",
      "(81, 50, 6)\n",
      "(84, 50, 6)\n",
      "(87, 50, 6)\n",
      "(90, 50, 6)\n",
      "(93, 50, 6)\n",
      "(96, 50, 6)\n",
      "(99, 50, 6)\n",
      "(102, 50, 6)\n",
      "(105, 50, 6)\n",
      "(108, 50, 6)\n",
      "(111, 50, 6)\n",
      "(120, 50, 6)\n",
      "(125, 50, 6)\n",
      "(134, 50, 6)\n",
      "(137, 50, 6)\n",
      "(140, 50, 6)\n",
      "(143, 50, 6)\n",
      "(146, 50, 6)\n",
      "(155, 50, 6)\n",
      "(159, 50, 6)\n",
      "(163, 50, 6)\n",
      "(168, 50, 6)\n",
      "(177, 50, 6)\n",
      "(180, 50, 6)\n",
      "(185, 50, 6)\n",
      "(194, 50, 6)\n",
      "(199, 50, 6)\n",
      "(202, 50, 6)\n",
      "(205, 50, 6)\n",
      "(210, 50, 6)\n",
      "(215, 50, 6)\n",
      "(218, 50, 6)\n",
      "(225, 50, 6)\n",
      "(228, 50, 6)\n",
      "(231, 50, 6)\n",
      "(236, 50, 6)\n",
      "(238, 50, 6)\n",
      "(241, 50, 6)\n",
      "(248, 50, 6)\n",
      "(251, 50, 6)\n",
      "(254, 50, 6)\n",
      "(261, 50, 6)\n",
      "(268, 50, 6)\n",
      "(270, 50, 6)\n",
      "(272, 50, 6)\n",
      "(280, 50, 6)\n",
      "(285, 50, 6)\n",
      "(288, 50, 6)\n",
      "(293, 50, 6)\n",
      "(296, 50, 6)\n",
      "(300, 50, 6)\n",
      "(308, 50, 6)\n",
      "(312, 50, 6)\n",
      "(315, 50, 6)\n",
      "(318, 50, 6)\n",
      "(326, 50, 6)\n",
      "(331, 50, 6)\n",
      "(334, 50, 6)\n",
      "(337, 50, 6)\n",
      "(340, 50, 6)\n",
      "(343, 50, 6)\n",
      "(351, 50, 6)\n",
      "(359, 50, 6)\n",
      "(362, 50, 6)\n",
      "(366, 50, 6)\n",
      "(373, 50, 6)\n",
      "(376, 50, 6)\n",
      "(379, 50, 6)\n",
      "(382, 50, 6)\n",
      "(385, 50, 6)\n",
      "(388, 50, 6)\n",
      "(391, 50, 6)\n",
      "(394, 50, 6)\n",
      "(399, 50, 6)\n",
      "(402, 50, 6)\n",
      "(411, 50, 6)\n",
      "(416, 50, 6)\n",
      "(419, 50, 6)\n",
      "(427, 50, 6)\n",
      "(430, 50, 6)\n",
      "(435, 50, 6)\n",
      "(440, 50, 6)\n",
      "(443, 50, 6)\n",
      "(451, 50, 6)\n",
      "(454, 50, 6)\n",
      "(463, 50, 6)\n",
      "(466, 50, 6)\n",
      "(468, 50, 6)\n",
      "(470, 50, 6)\n",
      "(472, 50, 6)\n",
      "(476, 50, 6)\n",
      "(478, 50, 6)\n",
      "(481, 50, 6)\n",
      "(484, 50, 6)\n",
      "(493, 50, 6)\n",
      "(498, 50, 6)\n",
      "(501, 50, 6)\n",
      "(504, 50, 6)\n",
      "(513, 50, 6)\n",
      "(517, 50, 6)\n",
      "(521, 50, 6)\n",
      "(525, 50, 6)\n",
      "(529, 50, 6)\n",
      "(532, 50, 6)\n",
      "(536, 50, 6)\n",
      "(541, 50, 6)\n",
      "(545, 50, 6)\n",
      "(548, 50, 6)\n",
      "(557, 50, 6)\n",
      "(560, 50, 6)\n",
      "(565, 50, 6)\n",
      "(568, 50, 6)\n",
      "(576, 50, 6)\n",
      "(584, 50, 6)\n",
      "(589, 50, 6)\n",
      "(597, 50, 6)\n",
      "(599, 50, 6)\n",
      "(602, 50, 6)\n",
      "(605, 50, 6)\n",
      "(607, 50, 6)\n",
      "(612, 50, 6)\n",
      "(615, 50, 6)\n",
      "(623, 50, 6)\n",
      "(631, 50, 6)\n",
      "(633, 50, 6)\n",
      "(638, 50, 6)\n",
      "(641, 50, 6)\n",
      "(649, 50, 6)\n",
      "(657, 50, 6)\n",
      "(660, 50, 6)\n",
      "(665, 50, 6)\n",
      "(668, 50, 6)\n",
      "(676, 50, 6)\n",
      "(679, 50, 6)\n",
      "(684, 50, 6)\n",
      "(689, 50, 6)\n",
      "(692, 50, 6)\n",
      "(694, 50, 6)\n",
      "(696, 50, 6)\n",
      "(699, 50, 6)\n",
      "(702, 50, 6)\n",
      "(704, 50, 6)\n",
      "(709, 50, 6)\n",
      "(718, 50, 6)\n",
      "(721, 50, 6)\n",
      "(724, 50, 6)\n",
      "(727, 50, 6)\n",
      "(730, 50, 6)\n",
      "(733, 50, 6)\n",
      "(742, 50, 6)\n",
      "(745, 50, 6)\n",
      "(754, 50, 6)\n",
      "(759, 50, 6)\n",
      "(762, 50, 6)\n",
      "(765, 50, 6)\n",
      "(768, 50, 6)\n",
      "(773, 50, 6)\n",
      "(778, 50, 6)\n",
      "(780, 50, 6)\n",
      "(783, 50, 6)\n",
      "(785, 50, 6)\n",
      "(794, 50, 6)\n",
      "(802, 50, 6)\n",
      "(805, 50, 6)\n",
      "(808, 50, 6)\n",
      "(811, 50, 6)\n",
      "(814, 50, 6)\n",
      "(817, 50, 6)\n",
      "(825, 50, 6)\n",
      "(829, 50, 6)\n",
      "(832, 50, 6)\n",
      "(840, 50, 6)\n",
      "(843, 50, 6)\n",
      "(846, 50, 6)\n",
      "(849, 50, 6)\n",
      "(852, 50, 6)\n",
      "(855, 50, 6)\n",
      "(859, 50, 6)\n",
      "(861, 50, 6)\n",
      "(864, 50, 6)\n",
      "(869, 50, 6)\n",
      "(872, 50, 6)\n",
      "(880, 50, 6)\n",
      "(888, 50, 6)\n",
      "(891, 50, 6)\n",
      "(899, 50, 6)\n",
      "(907, 50, 6)\n",
      "(916, 50, 6)\n",
      "(919, 50, 6)\n",
      "(922, 50, 6)\n",
      "(930, 50, 6)\n",
      "(933, 50, 6)\n",
      "(936, 50, 6)\n",
      "(939, 50, 6)\n",
      "(948, 50, 6)\n",
      "(953, 50, 6)\n",
      "(956, 50, 6)\n",
      "(959, 50, 6)\n",
      "(962, 50, 6)\n",
      "(965, 50, 6)\n",
      "(968, 50, 6)\n",
      "(971, 50, 6)\n",
      "(976, 50, 6)\n",
      "(981, 50, 6)\n",
      "(986, 50, 6)\n",
      "(989, 50, 6)\n",
      "(992, 50, 6)\n",
      "(995, 50, 6)\n",
      "(998, 50, 6)\n",
      "(1007, 50, 6)\n",
      "(1012, 50, 6)\n",
      "(1017, 50, 6)\n",
      "(1020, 50, 6)\n",
      "(1023, 50, 6)\n",
      "(1026, 50, 6)\n",
      "(1028, 50, 6)\n",
      "(1031, 50, 6)\n",
      "(1039, 50, 6)\n",
      "(1042, 50, 6)\n",
      "(1046, 50, 6)\n",
      "(1049, 50, 6)\n",
      "(1057, 50, 6)\n",
      "(1065, 50, 6)\n",
      "(1073, 50, 6)\n",
      "(1076, 50, 6)\n",
      "(1078, 50, 6)\n",
      "(1082, 50, 6)\n",
      "(1085, 50, 6)\n",
      "(1089, 50, 6)\n",
      "(1093, 50, 6)\n",
      "(1101, 50, 6)\n",
      "(1104, 50, 6)\n",
      "(1107, 50, 6)\n",
      "(1109, 50, 6)\n",
      "(1112, 50, 6)\n",
      "(1115, 50, 6)\n",
      "(1124, 50, 6)\n",
      "(1129, 50, 6)\n",
      "(1132, 50, 6)\n",
      "(1135, 50, 6)\n",
      "(1138, 50, 6)\n",
      "(1141, 50, 6)\n",
      "(1149, 50, 6)\n",
      "(1152, 50, 6)\n",
      "(1155, 50, 6)\n",
      "(1158, 50, 6)\n",
      "(1166, 50, 6)\n",
      "(1171, 50, 6)\n",
      "(1174, 50, 6)\n",
      "(1177, 50, 6)\n",
      "(1180, 50, 6)\n",
      "(1185, 50, 6)\n",
      "(1193, 50, 6)\n",
      "(1198, 50, 6)\n",
      "(1203, 50, 6)\n",
      "(1206, 50, 6)\n",
      "(1209, 50, 6)\n",
      "(1217, 50, 6)\n",
      "(1225, 50, 6)\n",
      "(1228, 50, 6)\n",
      "(1230, 50, 6)\n",
      "(1239, 50, 6)\n",
      "(1244, 50, 6)\n",
      "(1247, 50, 6)\n",
      "(1249, 50, 6)\n",
      "(1254, 50, 6)\n",
      "(1259, 50, 6)\n",
      "(1268, 50, 6)\n",
      "(1272, 50, 6)\n",
      "(1281, 50, 6)\n",
      "(1284, 50, 6)\n",
      "(1287, 50, 6)\n",
      "(1290, 50, 6)\n",
      "(1293, 50, 6)\n",
      "(1298, 50, 6)\n",
      "(1301, 50, 6)\n",
      "(1304, 50, 6)\n",
      "(1306, 50, 6)\n",
      "(1308, 50, 6)\n",
      "(1310, 50, 6)\n",
      "(1313, 50, 6)\n",
      "(1316, 50, 6)\n",
      "(1324, 50, 6)\n",
      "(1327, 50, 6)\n",
      "(1335, 50, 6)\n",
      "(1338, 50, 6)\n",
      "(1343, 50, 6)\n",
      "(1346, 50, 6)\n",
      "(1350, 50, 6)\n",
      "(1353, 50, 6)\n",
      "(1356, 50, 6)\n",
      "(1364, 50, 6)\n",
      "(1368, 50, 6)\n",
      "(1376, 50, 6)\n",
      "(1379, 50, 6)\n",
      "(1387, 50, 6)\n",
      "(1390, 50, 6)\n",
      "(1393, 50, 6)\n",
      "(1396, 50, 6)\n",
      "(1399, 50, 6)\n",
      "(1406, 50, 6)\n",
      "(1409, 50, 6)\n",
      "(1412, 50, 6)\n",
      "(1421, 50, 6)\n",
      "(1423, 50, 6)\n",
      "(1428, 50, 6)\n",
      "(1430, 50, 6)\n",
      "(1433, 50, 6)\n",
      "(1436, 50, 6)\n",
      "(1439, 50, 6)\n",
      "(1444, 50, 6)\n",
      "(1447, 50, 6)\n",
      "(1452, 50, 6)\n",
      "(1454, 50, 6)\n",
      "(1457, 50, 6)\n",
      "(1466, 50, 6)\n",
      "(1471, 50, 6)\n",
      "(1474, 50, 6)\n",
      "(1483, 50, 6)\n",
      "(1492, 50, 6)\n",
      "(1497, 50, 6)\n",
      "(1499, 50, 6)\n",
      "(1502, 50, 6)\n",
      "(1505, 50, 6)\n",
      "(1514, 50, 6)\n",
      "(1516, 50, 6)\n",
      "(1519, 50, 6)\n",
      "(1522, 50, 6)\n",
      "(1531, 50, 6)\n",
      "(1534, 50, 6)\n",
      "(1537, 50, 6)\n",
      "(1540, 50, 6)\n",
      "(1544, 50, 6)\n",
      "(1547, 50, 6)\n",
      "(1550, 50, 6)\n",
      "(1559, 50, 6)\n",
      "(1564, 50, 6)\n",
      "(1569, 50, 6)\n",
      "(1572, 50, 6)\n",
      "(1575, 50, 6)\n",
      "(1578, 50, 6)\n",
      "(1582, 50, 6)\n",
      "(1585, 50, 6)\n",
      "(1588, 50, 6)\n",
      "(1596, 50, 6)\n",
      "(1605, 50, 6)\n",
      "(1614, 50, 6)\n",
      "(1617, 50, 6)\n",
      "(1620, 50, 6)\n",
      "(1623, 50, 6)\n",
      "(1625, 50, 6)\n",
      "(1627, 50, 6)\n",
      "(1636, 50, 6)\n",
      "(1638, 50, 6)\n",
      "(1641, 50, 6)\n",
      "(1646, 50, 6)\n",
      "(1655, 50, 6)\n",
      "(1658, 50, 6)\n",
      "(1661, 50, 6)\n",
      "(1666, 50, 6)\n",
      "(1668, 50, 6)\n",
      "(1671, 50, 6)\n",
      "(1680, 50, 6)\n",
      "(1685, 50, 6)\n",
      "(1690, 50, 6)\n",
      "(1692, 50, 6)\n",
      "(1701, 50, 6)\n",
      "(1704, 50, 6)\n",
      "(1709, 50, 6)\n",
      "(1712, 50, 6)\n",
      "(1720, 50, 6)\n",
      "(1722, 50, 6)\n",
      "(1730, 50, 6)\n",
      "(1733, 50, 6)\n",
      "(1741, 50, 6)\n",
      "(1745, 50, 6)\n",
      "(1748, 50, 6)\n",
      "(1756, 50, 6)\n",
      "(1761, 50, 6)\n",
      "(1765, 50, 6)\n",
      "(1769, 50, 6)\n",
      "(1773, 50, 6)\n",
      "(1777, 50, 6)\n",
      "(1780, 50, 6)\n",
      "(1783, 50, 6)\n",
      "(1787, 50, 6)\n",
      "(1792, 50, 6)\n",
      "(1795, 50, 6)\n",
      "(1798, 50, 6)\n",
      "(1801, 50, 6)\n",
      "(1809, 50, 6)\n",
      "(1812, 50, 6)\n",
      "(1817, 50, 6)\n",
      "(1820, 50, 6)\n",
      "(1824, 50, 6)\n",
      "(1829, 50, 6)\n",
      "(1834, 50, 6)\n",
      "(1837, 50, 6)\n",
      "(1842, 50, 6)\n",
      "(1845, 50, 6)\n",
      "(1848, 50, 6)\n",
      "(1851, 50, 6)\n",
      "(1854, 50, 6)\n",
      "(1862, 50, 6)\n",
      "(1865, 50, 6)\n",
      "(1873, 50, 6)\n",
      "(1881, 50, 6)\n",
      "(1884, 50, 6)\n",
      "(1889, 50, 6)\n",
      "(1897, 50, 6)\n",
      "(1901, 50, 6)\n",
      "(1904, 50, 6)\n",
      "(1907, 50, 6)\n",
      "(1910, 50, 6)\n",
      "(1913, 50, 6)\n",
      "(1921, 50, 6)\n",
      "(1924, 50, 6)\n",
      "(1929, 50, 6)\n",
      "(1932, 50, 6)\n",
      "(1936, 50, 6)\n",
      "(1944, 50, 6)\n",
      "(1952, 50, 6)\n",
      "(1957, 50, 6)\n",
      "(1966, 50, 6)\n",
      "(1969, 50, 6)\n",
      "(1974, 50, 6)\n",
      "(1977, 50, 6)\n",
      "(1980, 50, 6)\n",
      "(1983, 50, 6)\n",
      "(1986, 50, 6)\n",
      "(1989, 50, 6)\n",
      "(1994, 50, 6)\n",
      "(1997, 50, 6)\n",
      "(2000, 50, 6)\n",
      "(2003, 50, 6)\n",
      "(2007, 50, 6)\n",
      "(2010, 50, 6)\n",
      "(2018, 50, 6)\n",
      "(2022, 50, 6)\n",
      "(2025, 50, 6)\n",
      "(2028, 50, 6)\n",
      "(2031, 50, 6)\n",
      "(2036, 50, 6)\n",
      "(2039, 50, 6)\n",
      "(2049, 50, 6)\n",
      "(2057, 50, 6)\n",
      "(2065, 50, 6)\n",
      "(2070, 50, 6)\n",
      "(2073, 50, 6)\n",
      "(2078, 50, 6)\n",
      "(2081, 50, 6)\n",
      "(2084, 50, 6)\n",
      "(2089, 50, 6)\n",
      "(2092, 50, 6)\n",
      "(2095, 50, 6)\n",
      "(2098, 50, 6)\n",
      "(2101, 50, 6)\n",
      "(2106, 50, 6)\n",
      "(2114, 50, 6)\n",
      "(2117, 50, 6)\n",
      "(2120, 50, 6)\n",
      "(2128, 50, 6)\n",
      "(2131, 50, 6)\n",
      "(2134, 50, 6)\n",
      "(2138, 50, 6)\n",
      "(2141, 50, 6)\n",
      "(2145, 50, 6)\n",
      "(2149, 50, 6)\n",
      "(2153, 50, 6)\n",
      "(2161, 50, 6)\n",
      "(2164, 50, 6)\n",
      "(2168, 50, 6)\n",
      "(2173, 50, 6)\n",
      "(2176, 50, 6)\n",
      "(2179, 50, 6)\n",
      "(2183, 50, 6)\n",
      "(2191, 50, 6)\n",
      "(2194, 50, 6)\n",
      "(2202, 50, 6)\n",
      "(2205, 50, 6)\n",
      "(2213, 50, 6)\n",
      "(2216, 50, 6)\n",
      "(2224, 50, 6)\n",
      "(2227, 50, 6)\n",
      "(2237, 50, 6)\n",
      "(2246, 50, 6)\n",
      "(2251, 50, 6)\n",
      "(2254, 50, 6)\n",
      "(2257, 50, 6)\n",
      "(2261, 50, 6)\n",
      "(2264, 50, 6)\n",
      "(2269, 50, 6)\n",
      "(2272, 50, 6)\n",
      "(2275, 50, 6)\n",
      "(2286, 50, 6)\n",
      "(2288, 50, 6)\n",
      "(2290, 50, 6)\n",
      "(2293, 50, 6)\n",
      "(2295, 50, 6)\n",
      "(2298, 50, 6)\n",
      "(2306, 50, 6)\n",
      "(2316, 50, 6)\n",
      "(2319, 50, 6)\n",
      "(2321, 50, 6)\n",
      "(2326, 50, 6)\n",
      "(2329, 50, 6)\n",
      "(2334, 50, 6)\n",
      "(2337, 50, 6)\n",
      "(2340, 50, 6)\n",
      "(2344, 50, 6)\n",
      "(2347, 50, 6)\n",
      "(2351, 50, 6)\n",
      "(2360, 50, 6)\n",
      "(2365, 50, 6)\n",
      "(2368, 50, 6)\n",
      "(2371, 50, 6)\n",
      "(2374, 50, 6)\n",
      "(2384, 50, 6)\n",
      "(2388, 50, 6)\n",
      "(2391, 50, 6)\n",
      "(2400, 50, 6)\n",
      "(2403, 50, 6)\n",
      "(2412, 50, 6)\n",
      "(2416, 50, 6)\n",
      "(2418, 50, 6)\n",
      "(2421, 50, 6)\n",
      "(2424, 50, 6)\n",
      "(2427, 50, 6)\n",
      "(2430, 50, 6)\n",
      "(2433, 50, 6)\n",
      "(2436, 50, 6)\n",
      "(2438, 50, 6)\n",
      "(2443, 50, 6)\n",
      "(2446, 50, 6)\n",
      "(2451, 50, 6)\n",
      "(2453, 50, 6)\n",
      "(2461, 50, 6)\n",
      "(2464, 50, 6)\n",
      "(2467, 50, 6)\n",
      "(2470, 50, 6)\n",
      "(2473, 50, 6)\n",
      "(2475, 50, 6)\n",
      "(2478, 50, 6)\n",
      "(2486, 50, 6)\n",
      "(2489, 50, 6)\n",
      "(2492, 50, 6)\n",
      "(2500, 50, 6)\n",
      "(2502, 50, 6)\n",
      "(2507, 50, 6)\n",
      "(2512, 50, 6)\n",
      "(2515, 50, 6)\n",
      "(2517, 50, 6)\n",
      "(2524, 50, 6)\n",
      "(2528, 50, 6)\n",
      "(2532, 50, 6)\n",
      "(2536, 50, 6)\n",
      "(2539, 50, 6)\n",
      "(2543, 50, 6)\n",
      "(2547, 50, 6)\n",
      "(2551, 50, 6)\n",
      "(2555, 50, 6)\n",
      "(2558, 50, 6)\n",
      "(2561, 50, 6)\n",
      "(2571, 50, 6)\n",
      "(2578, 50, 6)\n",
      "(2582, 50, 6)\n",
      "(2586, 50, 6)\n",
      "(2589, 50, 6)\n",
      "(2596, 50, 6)\n",
      "(2607, 50, 6)\n",
      "(2610, 50, 6)\n",
      "(2620, 50, 6)\n",
      "(2627, 50, 6)\n",
      "(2637, 50, 6)\n",
      "(2646, 50, 6)\n",
      "(2648, 50, 6)\n",
      "(2651, 50, 6)\n",
      "(2653, 50, 6)\n",
      "(2656, 50, 6)\n",
      "(2658, 50, 6)\n",
      "(2661, 50, 6)\n",
      "(2666, 50, 6)\n",
      "(2675, 50, 6)\n",
      "(2684, 50, 6)\n",
      "(2688, 50, 6)\n",
      "(2693, 50, 6)\n",
      "(2695, 50, 6)\n",
      "(2698, 50, 6)\n",
      "(2701, 50, 6)\n",
      "(2704, 50, 6)\n",
      "(2707, 50, 6)\n",
      "(2716, 50, 6)\n",
      "(2719, 50, 6)\n",
      "(2728, 50, 6)\n",
      "(2731, 50, 6)\n",
      "(2734, 50, 6)\n",
      "(2739, 50, 6)\n",
      "(2741, 50, 6)\n",
      "(2746, 50, 6)\n",
      "(2749, 50, 6)\n",
      "(2752, 50, 6)\n",
      "(2755, 50, 6)\n",
      "(2764, 50, 6)\n",
      "(2773, 50, 6)\n",
      "(2778, 50, 6)\n",
      "(2787, 50, 6)\n",
      "(2792, 50, 6)\n",
      "(2795, 50, 6)\n",
      "(2800, 50, 6)\n",
      "(2803, 50, 6)\n",
      "(2806, 50, 6)\n",
      "(2811, 50, 6)\n",
      "(2814, 50, 6)\n",
      "(2817, 50, 6)\n",
      "(2820, 50, 6)\n",
      "(2825, 50, 6)\n",
      "(2834, 50, 6)\n",
      "(2843, 50, 6)\n",
      "(2847, 50, 6)\n",
      "(2850, 50, 6)\n",
      "(2853, 50, 6)\n",
      "(2857, 50, 6)\n",
      "(2860, 50, 6)\n",
      "(2868, 50, 6)\n",
      "(2876, 50, 6)\n",
      "(2880, 50, 6)\n",
      "(2883, 50, 6)\n",
      "(2887, 50, 6)\n",
      "(2890, 50, 6)\n",
      "(2893, 50, 6)\n",
      "(2898, 50, 6)\n",
      "(2906, 50, 6)\n",
      "(2909, 50, 6)\n",
      "(2914, 50, 6)\n",
      "(2918, 50, 6)\n",
      "(2923, 50, 6)\n",
      "(2926, 50, 6)\n",
      "(2929, 50, 6)\n",
      "(2937, 50, 6)\n",
      "(2941, 50, 6)\n",
      "(2946, 50, 6)\n",
      "(2954, 50, 6)\n",
      "(2957, 50, 6)\n",
      "(2962, 50, 6)\n",
      "(2965, 50, 6)\n",
      "(2967, 50, 6)\n",
      "(2972, 50, 6)\n",
      "(2975, 50, 6)\n",
      "(2977, 50, 6)\n",
      "(2979, 50, 6)\n",
      "(2983, 50, 6)\n",
      "(2986, 50, 6)\n",
      "(2989, 50, 6)\n",
      "(2992, 50, 6)\n",
      "(2995, 50, 6)\n",
      "(2997, 50, 6)\n",
      "(3000, 50, 6)\n",
      "(3008, 50, 6)\n",
      "(3011, 50, 6)\n",
      "(3019, 50, 6)\n",
      "(3026, 50, 6)\n",
      "(3030, 50, 6)\n",
      "(3038, 50, 6)\n",
      "(3042, 50, 6)\n",
      "(3050, 50, 6)\n",
      "(3054, 50, 6)\n",
      "(3056, 50, 6)\n",
      "(3059, 50, 6)\n",
      "(3062, 50, 6)\n",
      "(3065, 50, 6)\n",
      "(3068, 50, 6)\n",
      "(3073, 50, 6)\n",
      "(3076, 50, 6)\n",
      "(3078, 50, 6)\n",
      "(3083, 50, 6)\n",
      "(3087, 50, 6)\n",
      "(3090, 50, 6)\n",
      "(3093, 50, 6)\n",
      "(3098, 50, 6)\n",
      "(3101, 50, 6)\n",
      "(3110, 50, 6)\n",
      "(3115, 50, 6)\n",
      "(3117, 50, 6)\n",
      "(3125, 50, 6)\n",
      "(3128, 50, 6)\n",
      "(3136, 50, 6)\n",
      "(3139, 50, 6)\n",
      "(3147, 50, 6)\n",
      "(3149, 50, 6)\n",
      "(3151, 50, 6)\n",
      "(3153, 50, 6)\n",
      "(3158, 50, 6)\n",
      "(3163, 50, 6)\n",
      "(3166, 50, 6)\n",
      "(3174, 50, 6)\n",
      "(3177, 50, 6)\n",
      "(3180, 50, 6)\n",
      "(3188, 50, 6)\n",
      "(3193, 50, 6)\n",
      "(3195, 50, 6)\n",
      "(3204, 50, 6)\n",
      "(3213, 50, 6)\n",
      "(3221, 50, 6)\n",
      "(3224, 50, 6)\n",
      "(3226, 50, 6)\n",
      "(3229, 50, 6)\n",
      "(3232, 50, 6)\n",
      "(3235, 50, 6)\n",
      "(3237, 50, 6)\n",
      "(3240, 50, 6)\n",
      "(3249, 50, 6)\n",
      "(3252, 50, 6)\n",
      "(3261, 50, 6)\n",
      "(3266, 50, 6)\n",
      "(3269, 50, 6)\n",
      "(3272, 50, 6)\n",
      "(3281, 50, 6)\n",
      "(3284, 50, 6)\n",
      "(3287, 50, 6)\n",
      "(3292, 50, 6)\n",
      "(3295, 50, 6)\n",
      "(3298, 50, 6)\n",
      "(3301, 50, 6)\n",
      "(3306, 50, 6)\n",
      "(3311, 50, 6)\n",
      "(3316, 50, 6)\n",
      "(3319, 50, 6)\n",
      "(3328, 50, 6)\n",
      "(3331, 50, 6)\n",
      "(3334, 50, 6)\n",
      "(3343, 50, 6)\n",
      "(3346, 50, 6)\n",
      "(3349, 50, 6)\n",
      "(3352, 50, 6)\n",
      "(3361, 50, 6)\n",
      "(3363, 50, 6)\n",
      "(3366, 50, 6)\n",
      "(3371, 50, 6)\n",
      "(3374, 50, 6)\n",
      "(3377, 50, 6)\n",
      "(3380, 50, 6)\n",
      "(3383, 50, 6)\n",
      "(3391, 50, 6)\n",
      "(3394, 50, 6)\n",
      "(3399, 50, 6)\n",
      "(3407, 50, 6)\n",
      "(3412, 50, 6)\n",
      "(3415, 50, 6)\n",
      "(3418, 50, 6)\n",
      "(3421, 50, 6)\n",
      "(3424, 50, 6)\n",
      "(3427, 50, 6)\n",
      "(3435, 50, 6)\n",
      "(3438, 50, 6)\n",
      "(3443, 50, 6)\n",
      "(3452, 50, 6)\n",
      "(3457, 50, 6)\n",
      "(3466, 50, 6)\n",
      "(3469, 50, 6)\n",
      "(3472, 50, 6)\n",
      "(3476, 50, 6)\n",
      "(3479, 50, 6)\n",
      "(3484, 50, 6)\n",
      "(3487, 50, 6)\n",
      "(3490, 50, 6)\n",
      "(3494, 50, 6)\n",
      "(3497, 50, 6)\n",
      "(3500, 50, 6)\n",
      "(3508, 50, 6)\n",
      "(3511, 50, 6)\n",
      "(3520, 50, 6)\n",
      "(3529, 50, 6)\n",
      "(3532, 50, 6)\n",
      "(3535, 50, 6)\n",
      "(3538, 50, 6)\n",
      "(3541, 50, 6)\n",
      "(3545, 50, 6)\n",
      "(3547, 50, 6)\n",
      "(3555, 50, 6)\n",
      "(3558, 50, 6)\n",
      "(3563, 50, 6)\n",
      "(3566, 50, 6)\n",
      "(3570, 50, 6)\n",
      "(3574, 50, 6)\n",
      "(3577, 50, 6)\n",
      "(3580, 50, 6)\n",
      "(3583, 50, 6)\n",
      "(3591, 50, 6)\n",
      "(3594, 50, 6)\n",
      "(3597, 50, 6)\n",
      "(3600, 50, 6)\n",
      "(3603, 50, 6)\n",
      "(3605, 50, 6)\n",
      "(3613, 50, 6)\n",
      "(3616, 50, 6)\n",
      "(3618, 50, 6)\n",
      "(3622, 50, 6)\n",
      "(3631, 50, 6)\n",
      "(3634, 50, 6)\n",
      "(3638, 50, 6)\n",
      "(3646, 50, 6)\n",
      "(3649, 50, 6)\n",
      "(3654, 50, 6)\n",
      "(3662, 50, 6)\n",
      "(3665, 50, 6)\n",
      "(3672, 50, 6)\n",
      "(3679, 50, 6)\n",
      "(3686, 50, 6)\n",
      "(3689, 50, 6)\n",
      "(3694, 50, 6)\n",
      "(3697, 50, 6)\n",
      "(3700, 50, 6)\n",
      "(3705, 50, 6)\n",
      "(3708, 50, 6)\n",
      "(3711, 50, 6)\n",
      "(3714, 50, 6)\n",
      "(3719, 50, 6)\n",
      "(3722, 50, 6)\n",
      "(3727, 50, 6)\n",
      "(3730, 50, 6)\n",
      "(3733, 50, 6)\n",
      "(3740, 50, 6)\n",
      "(3743, 50, 6)\n",
      "(3746, 50, 6)\n",
      "(3749, 50, 6)\n",
      "(3752, 50, 6)\n",
      "(3761, 50, 6)\n",
      "(3764, 50, 6)\n",
      "(3767, 50, 6)\n",
      "(3770, 50, 6)\n",
      "(3779, 50, 6)\n",
      "(3782, 50, 6)\n",
      "(3787, 50, 6)\n",
      "(3790, 50, 6)\n",
      "(3793, 50, 6)\n",
      "(3796, 50, 6)\n",
      "(3799, 50, 6)\n",
      "(3804, 50, 6)\n",
      "(3813, 50, 6)\n",
      "(3816, 50, 6)\n",
      "(3819, 50, 6)\n",
      "(3822, 50, 6)\n",
      "(3827, 50, 6)\n",
      "(3837, 50, 6)\n",
      "(3842, 50, 6)\n",
      "(3845, 50, 6)\n",
      "(3848, 50, 6)\n",
      "(3850, 50, 6)\n",
      "(3858, 50, 6)\n",
      "(3860, 50, 6)\n",
      "(3868, 50, 6)\n",
      "(3873, 50, 6)\n",
      "(3878, 50, 6)\n",
      "(3881, 50, 6)\n",
      "(3884, 50, 6)\n",
      "(3886, 50, 6)\n",
      "(3889, 50, 6)\n",
      "(3892, 50, 6)\n",
      "(3894, 50, 6)\n",
      "(3899, 50, 6)\n",
      "(3907, 50, 6)\n",
      "(3915, 50, 6)\n",
      "(3920, 50, 6)\n",
      "(3923, 50, 6)\n",
      "(3932, 50, 6)\n",
      "(3935, 50, 6)\n",
      "(3938, 50, 6)\n",
      "(3941, 50, 6)\n",
      "(3944, 50, 6)\n",
      "(3949, 50, 6)\n",
      "(3952, 50, 6)\n",
      "(3957, 50, 6)\n",
      "(3960, 50, 6)\n",
      "(3968, 50, 6)\n",
      "(3976, 50, 6)\n",
      "(3979, 50, 6)\n",
      "(3987, 50, 6)\n",
      "(3990, 50, 6)\n",
      "(3993, 50, 6)\n",
      "(3996, 50, 6)\n",
      "(3999, 50, 6)\n",
      "(4002, 50, 6)\n",
      "(4010, 50, 6)\n",
      "(4013, 50, 6)\n",
      "(4018, 50, 6)\n",
      "(4021, 50, 6)\n",
      "(4024, 50, 6)\n",
      "(4027, 50, 6)\n",
      "(4029, 50, 6)\n",
      "(4037, 50, 6)\n",
      "(4040, 50, 6)\n",
      "(4045, 50, 6)\n",
      "(4050, 50, 6)\n",
      "(4058, 50, 6)\n",
      "(4061, 50, 6)\n",
      "(4069, 50, 6)\n",
      "(4072, 50, 6)\n",
      "(4077, 50, 6)\n",
      "(4079, 50, 6)\n",
      "(4084, 50, 6)\n",
      "(4089, 50, 6)\n",
      "(4091, 50, 6)\n",
      "(4094, 50, 6)\n",
      "(4096, 50, 6)\n",
      "(4105, 50, 6)\n",
      "(4113, 50, 6)\n",
      "(4116, 50, 6)\n",
      "(4121, 50, 6)\n",
      "(4124, 50, 6)\n",
      "(4127, 50, 6)\n",
      "(4129, 50, 6)\n",
      "(4137, 50, 6)\n",
      "(4142, 50, 6)\n",
      "(4145, 50, 6)\n",
      "(4147, 50, 6)\n",
      "(4150, 50, 6)\n",
      "(4153, 50, 6)\n",
      "(4156, 50, 6)\n",
      "(4164, 50, 6)\n",
      "(4167, 50, 6)\n",
      "(4170, 50, 6)\n",
      "(4173, 50, 6)\n",
      "(4176, 50, 6)\n",
      "(4179, 50, 6)\n",
      "(4184, 50, 6)\n",
      "(4192, 50, 6)\n",
      "(4195, 50, 6)\n",
      "(4203, 50, 6)\n",
      "(4206, 50, 6)\n",
      "(4211, 50, 6)\n",
      "(4214, 50, 6)\n",
      "(4219, 50, 6)\n",
      "(4222, 50, 6)\n",
      "(4230, 50, 6)\n",
      "(4238, 50, 6)\n",
      "(4241, 50, 6)\n",
      "(4244, 50, 6)\n",
      "(4249, 50, 6)\n",
      "(4252, 50, 6)\n",
      "(4255, 50, 6)\n",
      "(4263, 50, 6)\n",
      "(4269, 50, 6)\n",
      "(4271, 50, 6)\n",
      "(4273, 50, 6)\n",
      "(4276, 50, 6)\n",
      "(4287, 50, 6)\n",
      "(4290, 50, 6)\n",
      "(4293, 50, 6)\n",
      "(4296, 50, 6)\n",
      "(4299, 50, 6)\n",
      "(4304, 50, 6)\n",
      "(4306, 50, 6)\n",
      "(4315, 50, 6)\n",
      "(4318, 50, 6)\n",
      "(4321, 50, 6)\n",
      "(4326, 50, 6)\n",
      "(4336, 50, 6)\n",
      "(4343, 50, 6)\n",
      "(4346, 50, 6)\n",
      "(4349, 50, 6)\n",
      "(4351, 50, 6)\n",
      "(4353, 50, 6)\n",
      "(4361, 50, 6)\n",
      "(4364, 50, 6)\n",
      "(4371, 50, 6)\n",
      "(4374, 50, 6)\n",
      "(4383, 50, 6)\n",
      "(4387, 50, 6)\n",
      "(4390, 50, 6)\n",
      "(4393, 50, 6)\n",
      "(4396, 50, 6)\n",
      "(4401, 50, 6)\n",
      "(4404, 50, 6)\n",
      "(4407, 50, 6)\n",
      "(4412, 50, 6)\n",
      "(4415, 50, 6)\n",
      "(4420, 50, 6)\n",
      "(4424, 50, 6)\n",
      "(4427, 50, 6)\n",
      "(4432, 50, 6)\n",
      "(4437, 50, 6)\n",
      "(4440, 50, 6)\n",
      "(4449, 50, 6)\n",
      "(4458, 50, 6)\n",
      "(4461, 50, 6)\n",
      "(4471, 50, 6)\n",
      "(4474, 50, 6)\n",
      "(4483, 50, 6)\n",
      "(4486, 50, 6)\n",
      "(4489, 50, 6)\n",
      "(4492, 50, 6)\n",
      "(4495, 50, 6)\n",
      "(4503, 50, 6)\n",
      "(4506, 50, 6)\n",
      "(4514, 50, 6)\n",
      "(4517, 50, 6)\n",
      "(4525, 50, 6)\n",
      "(4528, 50, 6)\n",
      "(4533, 50, 6)\n",
      "(4536, 50, 6)\n",
      "(4544, 50, 6)\n",
      "(4549, 50, 6)\n",
      "(4552, 50, 6)\n",
      "(4555, 50, 6)\n",
      "(4560, 50, 6)\n",
      "(4563, 50, 6)\n",
      "(4568, 50, 6)\n",
      "(4576, 50, 6)\n",
      "(4579, 50, 6)\n",
      "(4584, 50, 6)\n",
      "(4587, 50, 6)\n",
      "(4590, 50, 6)\n",
      "(4593, 50, 6)\n",
      "(4596, 50, 6)\n",
      "(4600, 50, 6)\n",
      "(4603, 50, 6)\n",
      "(4606, 50, 6)\n",
      "(4615, 50, 6)\n",
      "(4624, 50, 6)\n",
      "(4627, 50, 6)\n",
      "(4631, 50, 6)\n",
      "(4634, 50, 6)\n",
      "(4639, 50, 6)\n",
      "(4642, 50, 6)\n",
      "(4646, 50, 6)\n",
      "(4649, 50, 6)\n",
      "(4654, 50, 6)\n",
      "(4663, 50, 6)\n",
      "(4666, 50, 6)\n",
      "(4671, 50, 6)\n",
      "(4674, 50, 6)\n",
      "(4677, 50, 6)\n",
      "(4682, 50, 6)\n",
      "(4690, 50, 6)\n",
      "(4693, 50, 6)\n",
      "(4702, 50, 6)\n",
      "(4707, 50, 6)\n",
      "(4709, 50, 6)\n",
      "(4712, 50, 6)\n",
      "(4720, 50, 6)\n",
      "(4725, 50, 6)\n",
      "(4727, 50, 6)\n",
      "(4732, 50, 6)\n",
      "(4735, 50, 6)\n",
      "(4743, 50, 6)\n",
      "(4746, 50, 6)\n",
      "(4749, 50, 6)\n",
      "(4754, 50, 6)\n",
      "(4762, 50, 6)\n",
      "(4770, 50, 6)\n",
      "(4772, 50, 6)\n",
      "(4775, 50, 6)\n",
      "(4778, 50, 6)\n",
      "(4781, 50, 6)\n",
      "(4789, 50, 6)\n",
      "(4792, 50, 6)\n",
      "(4795, 50, 6)\n",
      "(4798, 50, 6)\n",
      "(4803, 50, 6)\n",
      "(4807, 50, 6)\n",
      "(4810, 50, 6)\n",
      "(4818, 50, 6)\n",
      "(4822, 50, 6)\n",
      "(4830, 50, 6)\n",
      "(4833, 50, 6)\n",
      "(4836, 50, 6)\n",
      "(4839, 50, 6)\n",
      "(4842, 50, 6)\n",
      "(4846, 50, 6)\n",
      "(4855, 50, 6)\n",
      "(4859, 50, 6)\n",
      "(4863, 50, 6)\n",
      "(4871, 50, 6)\n",
      "(4875, 50, 6)\n",
      "(4880, 50, 6)\n",
      "(4885, 50, 6)\n",
      "(4889, 50, 6)\n",
      "(4897, 50, 6)\n",
      "(4902, 50, 6)\n",
      "(4906, 50, 6)\n",
      "(4909, 50, 6)\n",
      "(4912, 50, 6)\n",
      "(4916, 50, 6)\n",
      "(4921, 50, 6)\n",
      "(4925, 50, 6)\n",
      "(4934, 50, 6)\n",
      "(4939, 50, 6)\n",
      "(4941, 50, 6)\n",
      "(4944, 50, 6)\n",
      "(4947, 50, 6)\n",
      "(4950, 50, 6)\n",
      "(4959, 50, 6)\n",
      "(4968, 50, 6)\n",
      "(4971, 50, 6)\n",
      "(4974, 50, 6)\n",
      "(4977, 50, 6)\n",
      "(4980, 50, 6)\n",
      "(4982, 50, 6)\n",
      "(4987, 50, 6)\n",
      "(4989, 50, 6)\n",
      "(4998, 50, 6)\n",
      "(5003, 50, 6)\n",
      "(5012, 50, 6)\n",
      "(5020, 50, 6)\n",
      "(5025, 50, 6)\n",
      "(5028, 50, 6)\n",
      "(5031, 50, 6)\n",
      "(5034, 50, 6)\n",
      "(5037, 50, 6)\n",
      "(5045, 50, 6)\n",
      "(5050, 50, 6)\n",
      "(5058, 50, 6)\n",
      "(5061, 50, 6)\n",
      "(5066, 50, 6)\n",
      "(5069, 50, 6)\n",
      "(5072, 50, 6)\n",
      "(5075, 50, 6)\n",
      "(5078, 50, 6)\n",
      "(5086, 50, 6)\n",
      "(5089, 50, 6)\n",
      "(5092, 50, 6)\n",
      "(5100, 50, 6)\n",
      "(5103, 50, 6)\n",
      "(5108, 50, 6)\n",
      "(5111, 50, 6)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "gait_duration = 2\n",
    "folders = [d for d in os.listdir(os.path.join(os.getcwd(),'dataset')) if os.path.isdir(os.path.join(os.path.join(os.getcwd(),'dataset'), d))]\n",
    "k=0\n",
    "if os.path.exists(f\"gait time series data\") == False:\n",
    "    os.mkdir(f\"gait time series data\")\n",
    "total_joint_states = np.empty((0,6))\n",
    "slide_window_size = 50 # Equate to 0.5 second\n",
    "window_data = np.empty((0,slide_window_size,6))\n",
    "for folder in folders:\n",
    "    folder_len = len(folder)\n",
    "    files = [f for f in os.listdir(os.path.join(os.getcwd(),'dataset',folder)) if f.endswith('.c3d')]\n",
    "    for file in files:\n",
    "        if 'ST' not in file:\n",
    "            c = c3d(os.path.join('dataset',folder,file))\n",
    "            joint_states = extract_gait_cycle(c,gait_duration=gait_duration,mode='time')\n",
    "            for i in range(0,joint_states.shape[0]-slide_window_size+1,slide_window_size):\n",
    "                window_data = np.concatenate((window_data, joint_states[i:i+slide_window_size,:][np.newaxis, ...]), axis=0)\n",
    "            print(window_data.shape)\n",
    "np.save(f\"gait time series data/window_data.npy\",window_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gait_duration = 2\n",
    "folders = [d for d in os.listdir(os.path.join(os.getcwd(),'dataset')) if os.path.isdir(os.path.join(os.path.join(os.getcwd(),'dataset'), d))]\n",
    "k=0\n",
    "if os.path.exists(f\"gait reference fft_plots_\") == False:\n",
    "    os.mkdir(f\"gait reference fft_plots_\")\n",
    "\n",
    "for folder in folders:\n",
    "    folder_len = len(folder)\n",
    "    files = [f for f in os.listdir(os.path.join(os.getcwd(),'dataset',folder)) if f.endswith('.c3d')]\n",
    "    \n",
    "    print(f\"{k}/{len(folders)} is processed current folder is {folder}\") \n",
    "    i = 0\n",
    "    for file in files:\n",
    "        if 'ST' not in file:\n",
    "            c = c3d(os.path.join('dataset',folder,file))\n",
    "            joint_states_rfft, freq_values, encoder_vec, joint_states = extract_gait_cycle(c,gait_duration=gait_duration)\n",
    "            # if np.max(np.abs(joint_states)) > 1:\n",
    "            #     print(f'joint_states max value is greater than 1 in {folder}_{file[:-4]}')\n",
    "            #     break\n",
    "            if i == 0:\n",
    "                folder_output_state = joint_states_rfft\n",
    "                folder_input_vector = encoder_vec\n",
    "            else:\n",
    "                folder_output_state = np.vstack((folder_output_state,joint_states_rfft))\n",
    "                folder_input_vector = np.vstack((folder_input_vector,encoder_vec))\n",
    "            i+=1\n",
    "    print('folder_output_state_shape: ',folder_output_state.shape)\n",
    "    print('folder input vector shape: ',folder_input_vector.shape)\n",
    "    print('-----------------------------------')\n",
    "    if k == 0:\n",
    "        total_output_state = folder_output_state\n",
    "        total_input_vector = folder_input_vector\n",
    "    else:\n",
    "        total_output_state = np.vstack((total_output_state,folder_output_state))\n",
    "        total_input_vector = np.vstack((total_input_vector,folder_input_vector))\n",
    "    k+=1\n",
    "#     np.save(f\"ref_gait_library_duration{gait_duration}/{folder}_output_state.npy\",folder_output_state)\n",
    "#     np.save(f\"ref_gait_library_duration{gait_duration}/{folder}_input_vector.npy\",folder_input_vector)\n",
    "\n",
    "if os.path.exists(f\"gait reference fft{freq_values[-1]:.2f}\") == False:\n",
    "    os.mkdir(f\"gait reference fft{freq_values[-1]:.2f}\")\n",
    "\n",
    "np.save(f\"gait reference fft{freq_values[-1]:.2f}/output_fft_constants.npy\",total_output_state)\n",
    "np.save(f\"gait reference fft{freq_values[-1]:.2f}/input_vector.npy\",total_input_vector)\n",
    "\n",
    "print('total output state ',total_output_state.shape)\n",
    "print('total input vector ',total_input_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**New Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(np.abs(total_input_vector[:,0])))  # speed\n",
    "print(np.max(np.abs(total_input_vector[:,1])))  # r_leglength\n",
    "print(np.max(np.abs(total_input_vector[:,2])))  # l_leglength\n",
    "\n",
    "print(total_output_state.shape)                 #shape of the vector\n",
    "\n",
    "r1_coeff = np.max(np.abs(total_output_state[:,0,:,0]))\n",
    "i1_coeff = np.max(np.abs(total_output_state[:,0,:,1]))\n",
    "r2_coeff = np.max(np.abs(total_output_state[:,1,:,0]))\n",
    "i2_coeff = np.max(np.abs(total_output_state[:,1,:,1]))\n",
    "r3_coeff = np.max(np.abs(total_output_state[:,2,:,0]))\n",
    "i3_coeff = np.max(np.abs(total_output_state[:,2,:,1]))\n",
    "r4_coeff = np.max(np.abs(total_output_state[:,3,:,0]))\n",
    "i4_coeff = np.max(np.abs(total_output_state[:,3,:,1]))\n",
    "r5_coeff = np.max(np.abs(total_output_state[:,4,:,0]))\n",
    "i5_coeff = np.max(np.abs(total_output_state[:,4,:,1]))\n",
    "r6_coeff = np.max(np.abs(total_output_state[:,5,:,0]))\n",
    "i6_coeff = np.max(np.abs(total_output_state[:,5,:,1]))\n",
    "r7_coeff = np.max(np.abs(total_output_state[:,6,:,0]))\n",
    "i7_coeff = np.max(np.abs(total_output_state[:,6,:,1]))\n",
    "r8_coeff = np.max(np.abs(total_output_state[:,7,:,0]))\n",
    "i8_coeff = np.max(np.abs(total_output_state[:,7,:,1]))\n",
    "r9_coeff = np.max(np.abs(total_output_state[:,8,:,0]))\n",
    "i9_coeff = np.max(np.abs(total_output_state[:,8,:,1]))\n",
    "r10_coeff = np.max(np.abs(total_output_state[:,9,:,0]))\n",
    "i10_coeff = np.max(np.abs(total_output_state[:,9,:,1]))\n",
    "r11_coeff = np.max(np.abs(total_output_state[:,10,:,0]))\n",
    "i11_coeff = np.max(np.abs(total_output_state[:,10,:,1]))\n",
    "r12_coeff = np.max(np.abs(total_output_state[:,11,:,0]))\n",
    "i12_coeff = np.max(np.abs(total_output_state[:,11,:,1]))\n",
    "r13_coeff = np.max(np.abs(total_output_state[:,12,:,0]))\n",
    "i13_coeff = np.max(np.abs(total_output_state[:,12,:,1]))\n",
    "r14_coeff = np.max(np.abs(total_output_state[:,13,:,0]))\n",
    "i14_coeff = np.max(np.abs(total_output_state[:,13,:,1]))\n",
    "r15_coeff = np.max(np.abs(total_output_state[:,14,:,0]))\n",
    "i15_coeff = np.max(np.abs(total_output_state[:,14,:,1]))\n",
    "r16_coeff = np.max(np.abs(total_output_state[:,15,:,0]))\n",
    "i16_coeff = np.max(np.abs(total_output_state[:,15,:,1]))\n",
    "r17_coeff = np.max(np.abs(total_output_state[:,16,:,0]))\n",
    "i17_coeff = np.max(np.abs(total_output_state[:,16,:,1]))\n",
    "\n",
    "print(f'r1_coeff: {r1_coeff} i1_coeff: {i1_coeff}')\n",
    "print(f'r2_coeff: {r2_coeff} i2_coeff: {i2_coeff}')\n",
    "print(f'r3_coeff: {r3_coeff} i3_coeff: {i3_coeff}')\n",
    "print(f'r4_coeff: {r4_coeff} i4_coeff: {i4_coeff}')\n",
    "print(f'r5_coeff: {r5_coeff} i5_coeff: {i5_coeff}')\n",
    "print(f'r6_coeff: {r6_coeff} i6_coeff: {i6_coeff}')\n",
    "print(f'r7_coeff: {r7_coeff} i7_coeff: {i7_coeff}')\n",
    "print(f'r8_coeff: {r8_coeff} i8_coeff: {i8_coeff}')\n",
    "print(f'r9_coeff: {r9_coeff} i9_coeff: {i9_coeff}')\n",
    "print(f'r10_coeff: {r10_coeff} i10_coeff: {i10_coeff}')\n",
    "print(f'r11_coeff: {r11_coeff} i11_coeff: {i11_coeff}')\n",
    "print(f'r12_coeff: {r12_coeff} i12_coeff: {i12_coeff}')\n",
    "print(f'r13_coeff: {r13_coeff} i13_coeff: {i13_coeff}')\n",
    "print(f'r14_coeff: {r14_coeff} i14_coeff: {i14_coeff}')\n",
    "print(f'r15_coeff: {r15_coeff} i15_coeff: {i15_coeff}')\n",
    "print(f'r16_coeff: {r16_coeff} i16_coeff: {i16_coeff}')\n",
    "print(f'r17_coeff: {r17_coeff} i17_coeff: {i17_coeff}')\n",
    "\n",
    "total_output_state[:,0,:,0] = total_output_state[:,0,:,0]/r1_coeff\n",
    "total_output_state[:,0,:,1] = total_output_state[:,0,:,1]\n",
    "total_output_state[:,1,:,0] = total_output_state[:,1,:,0]/r2_coeff\n",
    "total_output_state[:,1,:,1] = total_output_state[:,1,:,1]/i2_coeff\n",
    "total_output_state[:,2,:,0] = total_output_state[:,2,:,0]/r3_coeff\n",
    "total_output_state[:,2,:,1] = total_output_state[:,2,:,1]/i3_coeff\n",
    "total_output_state[:,3,:,0] = total_output_state[:,3,:,0]/r4_coeff\n",
    "total_output_state[:,3,:,1] = total_output_state[:,3,:,1]/i4_coeff\n",
    "total_output_state[:,4,:,0] = total_output_state[:,4,:,0]/r5_coeff\n",
    "total_output_state[:,4,:,1] = total_output_state[:,4,:,1]/i5_coeff\n",
    "total_output_state[:,5,:,0] = total_output_state[:,5,:,0]/r6_coeff\n",
    "total_output_state[:,5,:,1] = total_output_state[:,5,:,1]/i6_coeff\n",
    "total_output_state[:,6,:,0] = total_output_state[:,6,:,0]/r7_coeff\n",
    "total_output_state[:,6,:,1] = total_output_state[:,6,:,1]/i7_coeff\n",
    "total_output_state[:,7,:,0] = total_output_state[:,7,:,0]/r8_coeff\n",
    "total_output_state[:,7,:,1] = total_output_state[:,7,:,1]/i8_coeff\n",
    "total_output_state[:,8,:,0] = total_output_state[:,8,:,0]/r9_coeff\n",
    "total_output_state[:,8,:,1] = total_output_state[:,8,:,1]/i9_coeff\n",
    "total_output_state[:,9,:,0] = total_output_state[:,9,:,0]/r10_coeff\n",
    "total_output_state[:,9,:,1] = total_output_state[:,9,:,1]/i10_coeff\n",
    "total_output_state[:,10,:,0] = total_output_state[:,10,:,0]/r11_coeff\n",
    "total_output_state[:,10,:,1] = total_output_state[:,10,:,1]/i11_coeff\n",
    "total_output_state[:,11,:,0] = total_output_state[:,11,:,0]/r12_coeff\n",
    "total_output_state[:,11,:,1] = total_output_state[:,11,:,1]/i12_coeff\n",
    "total_output_state[:,12,:,0] = total_output_state[:,12,:,0]/r13_coeff\n",
    "total_output_state[:,12,:,1] = total_output_state[:,12,:,1]/i13_coeff\n",
    "total_output_state[:,13,:,0] = total_output_state[:,13,:,0]/r14_coeff\n",
    "total_output_state[:,13,:,1] = total_output_state[:,13,:,1]/i14_coeff\n",
    "total_output_state[:,14,:,0] = total_output_state[:,14,:,0]/r15_coeff\n",
    "total_output_state[:,14,:,1] = total_output_state[:,14,:,1]/i15_coeff\n",
    "total_output_state[:,15,:,0] = total_output_state[:,15,:,0]/r16_coeff\n",
    "total_output_state[:,15,:,1] = total_output_state[:,15,:,1]/i16_coeff\n",
    "total_output_state[:,16,:,0] = total_output_state[:,16,:,0]/r17_coeff\n",
    "total_output_state[:,16,:,1] = total_output_state[:,16,:,1]\n",
    "\n",
    "\n",
    "#i1 and i17 is zero for all the data points\n",
    "normalizationconst = [r1_coeff,i1_coeff,r2_coeff,i2_coeff,r3_coeff,i3_coeff,r4_coeff,i4_coeff,r5_coeff,i5_coeff,r6_coeff,i6_coeff,r7_coeff,i7_coeff,r8_coeff,i8_coeff,r9_coeff,i9_coeff,r10_coeff,i10_coeff,r11_coeff,i11_coeff,r12_coeff,i12_coeff,r13_coeff,i13_coeff,r14_coeff,i14_coeff,r15_coeff,i15_coeff,r16_coeff,i16_coeff,r17_coeff,i17_coeff]\n",
    "\n",
    "np.save(f\"gait reference fft{freq_values[-1]:.2f}/newnormalized_output_fft_constants.npy\",total_output_state)\n",
    "np.save(f\"gait reference fft{freq_values[-1]:.2f}/newnormalized_input_vector.npy\",total_input_vector)\n",
    "np.save(f\"gait reference fft{freq_values[-1]:.2f}/newnormalization_constants.npy\",normalizationconst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Old Way of Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NORMALIZATION OF THE DATA IS IMPLEMENTED\n",
    "# import numpy as np\n",
    "# real_rthigh = np.max(np.abs(total_output_state[:,:,0,0]))\n",
    "# imag_rthigh = np.max(np.abs(total_output_state[:,:,0,1]))\n",
    "\n",
    "# real_rshank = np.max(np.abs(total_output_state[:,:,1,0]))\n",
    "# imag_rshank = np.max(np.abs(total_output_state[:,:,1,1]))\n",
    "\n",
    "# real_lthigh = np.max(np.abs(total_output_state[:,:,2,0]))\n",
    "# imag_lthigh = np.max(np.abs(total_output_state[:,:,2,1]))\n",
    "\n",
    "# real_lshank = np.max(np.abs(total_output_state[:,:,3,0]))\n",
    "# imag_lshank = np.max(np.abs(total_output_state[:,:,3,1]))\n",
    "\n",
    "# print(real_rthigh,imag_rthigh,real_rshank,imag_rshank,real_lthigh,imag_lthigh,real_lshank,imag_lshank)\n",
    "\n",
    "# normalizationconst = np.array([real_rthigh,imag_rthigh,real_rshank,imag_rshank,real_lthigh,imag_lthigh,real_lshank,imag_lshank])\n",
    "\n",
    "# total_output_state[:,:,0,0] = total_output_state[:,:,0,0] / real_rthigh\n",
    "# total_output_state[:,:,1,0] = total_output_state[:,:,1,0] / real_rshank\n",
    "# total_output_state[:,:,2,0] = total_output_state[:,:,2,0] / real_lthigh\n",
    "# total_output_state[:,:,3,0] = total_output_state[:,:,3,0] / real_lshank\n",
    "\n",
    "# total_output_state[:,:,0,1] = total_output_state[:,:,0,1] / imag_rthigh\n",
    "# total_output_state[:,:,1,1] = total_output_state[:,:,1,1] / imag_rshank\n",
    "# total_output_state[:,:,2,1] = total_output_state[:,:,2,1] / imag_lthigh\n",
    "# total_output_state[:,:,3,1] = total_output_state[:,:,3,1] / imag_lshank\n",
    "\n",
    "# np.save(f\"gait reference fft{freq_values[-1]:.2f}/oldnormalized_output_fft_constants.npy\",total_output_state)\n",
    "# np.save(f\"gait reference fft{freq_values[-1]:.2f}/oldnormalized_input_vector.npy\",total_input_vector)\n",
    "# np.save(f\"gait reference fft{freq_values[-1]:.2f}/oldnormalization_constants.npy\",normalizationconst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_ifft_plots(joint_states_rfft,joint_states,frequencies,path,crop_ind=30):\n",
    "#     frequency = frequencies[crop_ind]\n",
    "#     joint_states_rfft = 180*np.squeeze(joint_states_rfft)\n",
    "#     cropped = joint_states_rfft.copy()\n",
    "#     cropped[crop_ind:,:] = 0\n",
    "#     ifft = np.fft.irfft(cropped, axis=0)\n",
    "#     cropped_ifft = ifft[56:-56,:]\n",
    "#     fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "#     axs[0, 0].plot(joint_states[:,0])\n",
    "#     axs[0, 0].plot(cropped_ifft[:,0])\n",
    "#     axs[0, 0].legend(['Original', 'Reconstructed'])\n",
    "#     axs[0, 0].set_title('Right Shank')\n",
    "#     axs[0, 1].plot(joint_states[:,1])\n",
    "#     axs[0, 1].plot(cropped_ifft[:,1])\n",
    "#     axs[0, 1].set_title('Right Thigh')\n",
    "#     axs[0, 1].legend(['Original', 'Reconstructed'])\n",
    "#     axs[1, 0].plot(joint_states[:,2])\n",
    "#     axs[1, 0].plot(cropped_ifft[:,2])\n",
    "#     axs[1, 0].set_title('Left Shank')\n",
    "#     axs[1, 0].legend(['Original', 'Reconstructed'])\n",
    "#     axs[1, 1].plot(joint_states[:,3])\n",
    "#     axs[1, 1].plot(cropped_ifft[:,3])\n",
    "#     axs[1, 1].set_title('Left Thigh')\n",
    "#     axs[1, 1].legend(['Original', 'Reconstructed'])\n",
    "#     plt.savefig(path+'.png')\n",
    "#     plt.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "train_for = \"normal\"\n",
    "\n",
    "inputs = np.load(rf\"gait reference fft5.00/newnormalized_input_vector.npy\")\n",
    "outputs = np.load(rf\"gait reference fft5.00/newnormalized_output_fft_constants.npy\")\n",
    "\n",
    "#train test split using sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=0.1, random_state=23)\n",
    "\n",
    "y_train = y_train\n",
    "y_train  = y_train.transpose(0,2,3,1)\n",
    "if train_for == \"taga\":\n",
    "    y_train = y_train[:,:4,:,:]\n",
    "print(y_train.shape)\n",
    "y_train = y_train.reshape(y_train.shape[0],-1)\n",
    "y_test = y_test\n",
    "y_test = y_test.transpose(0,2,3,1)\n",
    "if train_for == \"taga\":\n",
    "    y_test = y_test[:,:4,:,:]\n",
    "print(y_test.shape)\n",
    "y_test = y_test.reshape(y_test.shape[0],-1)\n",
    "#convert to tensor\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# create dataloader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "print('train data shape: ',X_train.shape)\n",
    "print('test data shape: ',X_test.shape)\n",
    "print('train output shape: ',y_train.shape)\n",
    "print('test output shape: ',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**New Normalize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(pred,gt,normalizationconst):\n",
    "    #form is [5,2,17]\n",
    "    for i in range(17):\n",
    "        for k in range(2):\n",
    "            pred[:,k,i] = pred[:,k,i] * normalizationconst[i*2+k]\n",
    "            gt[:,k,i] = gt[:,k,i] * normalizationconst[i*2+k]\n",
    "    \n",
    "    return pred,gt\n",
    "\n",
    "def pred_ifft(predictions,ground_truth,speed,normalizationconst):\n",
    "    #form is [5,2,17]\n",
    "    real_pred = predictions[:,0,:]\n",
    "    imag_pred = predictions[:,1,:]\n",
    "    predictions = real_pred + 1j*imag_pred\n",
    "    real_gt = ground_truth[:,0,:]\n",
    "    imag_gt = ground_truth[:,1,:]\n",
    "    ground_truth = real_gt + 1j*imag_gt\n",
    "    \n",
    "\n",
    "    pred_time = np.fft.irfft(predictions, axis=1)\n",
    "    gt_time = np.fft.irfft(ground_truth, axis=1)\n",
    "    pred_time = pred_time.transpose(1,0)\n",
    "    gt_time = gt_time.transpose(1,0)\n",
    "    #plot 2*2 subplots\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.subplot(2,3,1)\n",
    "    plt.plot(pred_time[:,0])\n",
    "    plt.plot(gt_time[:,0])\n",
    "    plt.title('Right Hip')\n",
    "    plt.legend(['Predicted','Ground Truth'])\n",
    "    plt.subplot(2,3,2)\n",
    "    plt.plot(pred_time[:,1])\n",
    "    plt.plot(gt_time[:,1])\n",
    "    plt.title('Right Knee')\n",
    "    plt.legend(['Predicted','Ground Truth'])\n",
    "    plt.subplot(2,3,3)\n",
    "    plt.plot(pred_time[:,2])\n",
    "    plt.plot(gt_time[:,2])\n",
    "    plt.title('Right Ankle')\n",
    "    plt.legend(['Predicted','Ground Truth'])\n",
    "    plt.subplot(2,3,4)\n",
    "    plt.plot(pred_time[:,3])\n",
    "    plt.plot(gt_time[:,3])\n",
    "    plt.title('Left Hip')\n",
    "    plt.legend(['Predicted','Ground Truth'])\n",
    "    plt.subplot(2,3,5)\n",
    "    plt.plot(pred_time[:,4])\n",
    "    plt.plot(gt_time[:,4])\n",
    "    plt.title('Left Knee')\n",
    "    plt.legend(['Predicted','Ground Truth'])\n",
    "    plt.subplot(2,3,6)\n",
    "    plt.plot(pred_time[:,5])\n",
    "    plt.plot(gt_time[:,5])\n",
    "    plt.title('Left Ankle')\n",
    "    plt.legend(['Predicted','Ground Truth'])\n",
    "\n",
    "    plt.savefig(f\"compare/newankle{speed:.2f}ms.png\")\n",
    "    plt.close()\n",
    "    print('plots saved')\n",
    "    return pred_time,gt_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Old Denormalize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def denormalize(pred,gt,normalizationconst):\n",
    "#     #form is [5,2,17]\n",
    "#     pred[0,0,:] = pred[0,0,:] * normalizationconst[0]\n",
    "#     pred[0,1,:] = pred[0,1,:] * normalizationconst[1]\n",
    "#     pred[1,0,:] = pred[1,0,:] * normalizationconst[2]\n",
    "#     pred[1,1,:] = pred[1,1,:] * normalizationconst[3]\n",
    "#     pred[2,0,:] = pred[2,0,:] * normalizationconst[4]\n",
    "#     pred[2,1,:] = pred[2,1,:] * normalizationconst[5]\n",
    "#     pred[3,0,:] = pred[3,0,:] * normalizationconst[6]\n",
    "#     pred[3,1,:] = pred[3,1,:] * normalizationconst[7]\n",
    "\n",
    "    \n",
    "\n",
    "#     gt[0,0,:] = gt[0,0,:] * normalizationconst[0]\n",
    "#     gt[0,1,:] = gt[0,1,:] * normalizationconst[1]\n",
    "#     gt[1,0,:] = gt[1,0,:] * normalizationconst[2]\n",
    "#     gt[1,1,:] = gt[1,1,:] * normalizationconst[3]\n",
    "#     gt[2,0,:] = gt[2,0,:] * normalizationconst[4]\n",
    "#     gt[2,1,:] = gt[2,1,:] * normalizationconst[5]\n",
    "#     gt[3,0,:] = gt[3,0,:] * normalizationconst[6]\n",
    "#     gt[3,1,:] = gt[3,1,:] * normalizationconst[7]\n",
    "\n",
    "    \n",
    "#     return pred,gt\n",
    "    \n",
    "\n",
    "# def pred_ifft(predictions,ground_truth,speed,normalizationconst):\n",
    "#     #form is [5,2,17]\n",
    "#     real_pred = predictions[:,0,:]\n",
    "#     imag_pred = predictions[:,1,:]\n",
    "#     predictions = real_pred + 1j*imag_pred\n",
    "#     real_gt = ground_truth[:,0,:]\n",
    "#     imag_gt = ground_truth[:,1,:]\n",
    "#     ground_truth = real_gt + 1j*imag_gt\n",
    "    \n",
    "\n",
    "#     pred_time = np.fft.irfft(predictions, axis=1)\n",
    "#     gt_time = np.fft.irfft(ground_truth, axis=1)\n",
    "#     pred_time = pred_time.transpose(1,0)\n",
    "#     gt_time = gt_time.transpose(1,0)\n",
    "#     #plot 2*2 subplots\n",
    "#     plt.figure(figsize=(10,8))\n",
    "#     plt.subplot(2,2,1)\n",
    "#     plt.plot(pred_time[:,0])\n",
    "#     plt.plot(gt_time[:,0])\n",
    "#     plt.title('Right Hip')\n",
    "#     plt.legend(['Predicted','Ground Truth'])\n",
    "#     plt.subplot(2,2,2)\n",
    "#     plt.plot(pred_time[:,1])\n",
    "#     plt.plot(gt_time[:,1])\n",
    "#     plt.title('Right Knee')\n",
    "#     plt.legend(['Predicted','Ground Truth'])\n",
    "#     plt.subplot(2,2,3)\n",
    "#     plt.plot(pred_time[:,2])\n",
    "#     plt.plot(gt_time[:,2])\n",
    "#     plt.title('Left Hip')\n",
    "#     plt.legend(['Predicted','Ground Truth'])\n",
    "#     plt.subplot(2,2,4)\n",
    "#     plt.plot(pred_time[:,3])\n",
    "#     plt.plot(gt_time[:,3])\n",
    "#     plt.title('Left Knee')\n",
    "#     plt.legend(['Predicted','Ground Truth'])\n",
    "\n",
    "#     plt.savefig(f\"compare/oldankle_{speed:.2f}ms.png\")\n",
    "#     plt.close()\n",
    "#     print('plots saved')\n",
    "#     return pred_time,gt_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Create the results folder if it does not exist.\n",
    "# -----------------------------\n",
    "\n",
    "results_dir = \"ref_gait_results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# # -----------------------------\n",
    "# # 1. Define Custom Loss Function\n",
    "# # -----------------------------\n",
    "# class CustomWeightedMSELoss(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CustomWeightedMSELoss, self).__init__()\n",
    "\n",
    "#     def forward(self, predictions, targets):\n",
    "#         # Compute element-wise squared error\n",
    "#         squared_error = (predictions - targets) ** 2\n",
    "#         # If the absolute prediction is larger than the absolute target, weight = 1.5; otherwise weight = 1.0.\n",
    "#         weights = torch.where(torch.abs(predictions) > torch.abs(targets),\n",
    "#                               torch.tensor(1.2, device=predictions.device),\n",
    "#                               torch.tensor(1.0, device=predictions.device))\n",
    "#         weighted_squared_error = weights * squared_error\n",
    "#         return torch.mean(weighted_squared_error)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Define Model Architecture\n",
    "# -----------------------------\n",
    "class SimpleFCNN(nn.Module):\n",
    "    def __init__(self, input_size=3, output_size=204, hidden_size=128):\n",
    "        super(SimpleFCNN, self).__init__()\n",
    "        # Layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [32,64]  # Different batch sizes to test\n",
    "hidden_sizes = [256,512]\n",
    "num_epochs_list = [8000]  # Number of epochs for training\n",
    "learning_rates = [1e-3,3e-4]\n",
    "input_size = 3\n",
    "output_size = 204\n",
    "\n",
    "torch.manual_seed(23)  # For reproducibility\n",
    "# This list will collect the results from each hyperparameter combination.\n",
    "tuning_results = []\n",
    "\n",
    "# Variables to store the best model and best validation loss\n",
    "best_val_loss = float('inf')\n",
    "best_params = None\n",
    "best_model_state = None\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Hyperparameter Tuning Loop\n",
    "# -----------------------------\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    # Create DataLoader for training and validation using the current batch size.\n",
    "    train_loader = DataLoader(train_data, batch_size=bs, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=bs, shuffle=False)\n",
    "    \n",
    "    for hs in hidden_sizes:\n",
    "        for num_epochs in num_epochs_list:\n",
    "            for lr in learning_rates:\n",
    "                print(f\"Training with batch_size={bs}, hidden_size={hs}, epochs={num_epochs}, lr={lr}\")\n",
    "                \n",
    "                # Initialize the model and optimizer for the current hyperparameters.\n",
    "                torch.manual_seed(23)  # Ensure reproducibility per run.\n",
    "                model = SimpleFCNN(input_size=input_size, output_size=output_size, hidden_size=hs)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "                loss_fn = nn.MSELoss()\n",
    "                \n",
    "                train_losses = []\n",
    "                val_losses = []\n",
    "                \n",
    "                # Training loop for the current hyperparameter combination.\n",
    "                for epoch in range(num_epochs):\n",
    "                    model.train()\n",
    "                    running_train_loss = 0.0\n",
    "                    for inputs, targets in train_loader:\n",
    "                        # Ensure targets are the right shape.\n",
    "                        targets = targets.view(-1, output_size)\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(inputs)\n",
    "                        loss = loss_fn(outputs, targets)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        running_train_loss += loss.item() * inputs.size(0)\n",
    "                    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "                    train_losses.append(epoch_train_loss)\n",
    "\n",
    "                    # Optionally print progress every 10 epochs (or at the last epoch)\n",
    "                    if (epoch+1) % 200 == 0 or epoch == num_epochs - 1:\n",
    "                        # Validation loop\n",
    "                        model.eval()\n",
    "                        running_val_loss = 0.0\n",
    "                        with torch.no_grad():\n",
    "                            for inputs, targets in val_loader:\n",
    "                                targets = targets.view(-1, output_size)\n",
    "                                outputs = model(inputs)\n",
    "                                loss = loss_fn(outputs, targets)\n",
    "                                running_val_loss += loss.item() * inputs.size(0)\n",
    "                        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "                        val_losses.append(epoch_val_loss)\n",
    "                        print(f\"  Epoch {epoch+1}/{num_epochs} | Train Loss: {epoch_train_loss:.6f} | Val Loss: {epoch_val_loss:.6f}\")\n",
    "                        # Record final losses from this run.\n",
    "                        final_train_loss = train_losses[-1]\n",
    "                        final_val_loss = val_losses[-1]\n",
    "                        result = {\n",
    "                            'batch_size': bs,\n",
    "                            'hidden_size': hs,\n",
    "                            'num_epochs': epoch+1,\n",
    "                            'learning_rate': lr,\n",
    "                            'final_train_loss': final_train_loss,\n",
    "                            'final_val_loss': final_val_loss\n",
    "                        }\n",
    "                        tuning_results.append(result)\n",
    "                        \n",
    "                        # Save the model if this run achieved the best validation loss so far.\n",
    "                        if final_val_loss < best_val_loss:\n",
    "                            best_lr = lr\n",
    "                            best_bs = bs\n",
    "                            best_hs = hs\n",
    "                            best_epoch = epoch+1\n",
    "                            best_val_loss = final_val_loss\n",
    "                            best_params = result\n",
    "                            best_model_state = model.state_dict()\n",
    "                            model_name = f\"best_model_hs{hs}_lr{lr}_bs{bs}_epochs{epoch}_val{final_val_loss}.pth\"\n",
    "                            torch.save(best_model_state, f\"best_model_hs{hs}_lr{lr}_bs{bs}_epochs{epoch}_val{final_val_loss}.pth\")\n",
    "                            print(f\"  New best model saved with val loss {best_val_loss:.6f}\")\n",
    "                            # Create a model filename that includes the hyperparameters and validation loss.\n",
    "                            # model_filename = f\"gaitgen_6d_newnorm_hs{hs}_lr{lr}_bs{bs}_epochs{num_epochs}_val{final_val_loss:.4f}.pth\"\n",
    "                            # torch.save(model.state_dict(), os.path.join(results_dir, model_filename))\n",
    "                            # print(f\"  New best model saved as {os.path.join(results_dir, model_filename)}\")\n",
    "\n",
    "                # (Optional) Save a plot of train/val losses for this hyperparameter setting.\n",
    "                plt.figure()\n",
    "                plt.plot(np.arange(1, num_epochs+1), train_losses, label='Train Loss')\n",
    "                plt.plot(np.linspace(10, num_epochs,num =len(val_losses)), val_losses, label='Val Loss')\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "                plt.title(f\"bs={bs}, hs={hs}, epochs={num_epochs}, lr={lr}\")\n",
    "                plt.tight_layout()\n",
    "                plot_filename = f\"loss_plot_bs{bs}_hs{hs}_epochs{num_epochs}_lr{lr}.png\"\n",
    "                plt.savefig(os.path.join(results_dir, plot_filename))\n",
    "                plt.close()\n",
    "                print(f\" Loss plot saved as {os.path.join(results_dir, plot_filename)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test dataset and DataLoader\n",
    "test_dataloader = DataLoader(val_data, batch_size=1, shuffle=False)  # Batch size of 5\n",
    "norm_consts = np.load(rf\"gait reference fft5.00/newnormalization_constants.npy\")\n",
    "# Testing loop\n",
    "model = SimpleFCNN(input_size=input_size, output_size=output_size, hidden_size=best_hs)\n",
    "model.load_state_dict(torch.load(model_name))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "test_loss = 0.0\n",
    "\n",
    "k = 0\n",
    "with torch.no_grad():  # No need to compute gradients during testing\n",
    "    for inputs, targets in test_dataloader:\n",
    "        speed = inputs[0,0].item()*3\n",
    "        targets = targets.reshape(-1,204)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        test_loss += loss.item()\n",
    "        ground_truth = targets.reshape(-1,6,2,17)\n",
    "        predictions = outputs.reshape(-1,6,2,17)\n",
    "\n",
    "        predictions = predictions.detach().numpy()\n",
    "        predictions = predictions[0]\n",
    "        ground_truth = ground_truth.detach().numpy()\n",
    "        ground_truth = ground_truth[0]\n",
    "\n",
    "        predictions, ground_truth = denormalize(predictions,ground_truth,norm_consts)\n",
    "        pred_time,gt_time = pred_ifft(predictions,ground_truth,speed,norm_consts)\n",
    "        # animate_biped(pred_time,f\"predict_plots_25hs512_fft/{speed:.1f}ms.gif\")\n",
    "        # animate_biped(gt_time,f\"predict_plots_25hs512_fft/{speed:.1f}ms.gif\")\n",
    "        k+=1\n",
    "# Calculate average loss and accuracy\n",
    "test_loss /= len(test_dataloader)  # Average test loss\n",
    "\n",
    "\n",
    "# Print test results\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "plt.plot(targets[0].numpy())\n",
    "plt.plot(outputs[0].numpy())\n",
    "plt.legend(['Ground Truth', 'Predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter Tuning is already completed no need to run the following cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.DataFrame(tuning_results)\n",
    "# results_df_filename = os.path.join(results_dir, \"gaitgen_6d_newnorm.csv\")\n",
    "# results_df.to_csv(results_df_filename, index=False)\n",
    "# print(\"Hyperparameter tuning complete.\")\n",
    "# print(\"Best hyperparameters:\")\n",
    "# print(best_params)\n",
    "# print(f\"Best model validation loss: {best_val_loss:.4f}\")\n",
    "# print(f\"Tuning results saved to {results_df_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the Network with the Full Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.load(rf\"gait reference fft5.00/newnormalized_input_vector.npy\")\n",
    "outputs = np.load(rf\"gait reference fft5.00/newnormalized_output_fft_constants.npy\")\n",
    "\n",
    "outputs = outputs.transpose(0,2,3,1)\n",
    "outputs = torch.tensor(outputs, dtype=torch.float32)\n",
    "outputs = outputs.reshape(outputs.shape[0],-1)\n",
    "inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "train_data = TensorDataset(inputs, outputs)\n",
    "\n",
    "# ADJUST THE PARAMETERS BASED ON THE BEST VALIDATION SCORED RESULT\n",
    "bs = best_bs\n",
    "hs = best_hs\n",
    "lr = best_lr\n",
    "train_loader = DataLoader(train_data, batch_size=bs, shuffle=True)\n",
    "\n",
    "num_epochs = best_epoch\n",
    "torch.manual_seed(23)  # Ensure reproducibility per run.\n",
    "model = SimpleFCNN(input_size=input_size, output_size=output_size, hidden_size=hs)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop for the current hyperparameter combination.\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        # Ensure targets are the right shape.\n",
    "        targets = targets.view(-1, output_size)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item() * inputs.size(0)\n",
    "    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    if (epoch+1) % 200 == 0 or epoch == num_epochs - 1:\n",
    "        print(f\"  Epoch {epoch+1}/{num_epochs} | Train Loss: {epoch_train_loss:.4f}\")\n",
    "\n",
    "model_filename = f\"final_model.pth\"\n",
    "torch.save(model.state_dict(), os.path.join(model_filename))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biped_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
