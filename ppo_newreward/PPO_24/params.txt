from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.callbacks import (
    CheckpointCallback,
    EvalCallback,
    StopTrainingOnNoModelImprovement,
    BaseCallback,
)
import torch
from ppoenv_guide import BipedEnv
import os
import datetime
from math import cos, pi
from stable_baselines3.common.callbacks import BaseCallback, CallbackList
from stable_baselines3 import PPO
import time
from typing import Callable
from utils import set_global_seed

set_global_seed(23)

t0 = time.time()
class RewardLoggerCallback(BaseCallback):
    def __init__(self, log_file: str, verbose: int = 0):
        super(RewardLoggerCallback, self).__init__(verbose)
        self.log_file = log_file
        self.episode_rewards = []
        self.current_episode_reward = 0
        self.current_step = 0  # Track the number of steps in the current episode

        # Create the log file if it doesn't exist
        if not os.path.exists(self.log_file):
            with open(self.log_file, 'w') as f:
                f.write("Episode,Total Reward,Termination Step\n")

    def _on_step(self) -> bool:
        # Check if the episode has ended
        dones = self.locals["dones"]
        rewards = self.locals["rewards"]

        # Accumulate rewards for the current episode
        self.current_episode_reward += rewards[0]
        self.current_step += 1  # Increment step count for the current episode

        # If the episode is done, log the reward and termination step
        if dones[0]:
            self.episode_rewards.append(self.current_episode_reward)
            with open(self.log_file, 'a') as f:
                f.write(f"{len(self.episode_rewards)},{self.current_episode_reward},{self.current_step}\n")
            
            # Reset counters for the next episode
            self.current_episode_reward = 0
            self.current_step = 0

        return True
    def _on_training_end(self) -> None:
        # Optionally summarize results at the end of training
        print("Training finished. Total episodes:", len(self.episode_rewards))

class CustomCheckpointCallback(BaseCallback):
    def __init__(self, save_freq, save_path,init_no = 0, verbose=0):
        super(CustomCheckpointCallback, self).__init__(verbose)
        self.save_freq = save_freq
        self.save_path = save_path
        self.init_no = init_no

    def _on_step(self) -> bool:
        done = False
        # Save the model every `save_freq` steps
        if self.n_calls % self.save_freq == 0:
            self.init_no +=1
            model_path = f"{self.save_path}/model_checkpoint_{self.init_no}"+checkpoint_name
            self.model.save(model_path)
            if self.verbose > 0:
                print(f"Model saved at step {self.n_calls} to {model_path}")
                print(f"Time taken for this checkpoint: {time.time() - t0:.2f} seconds")
                time.sleep(5)

        return True
    
def linear_schedule(initial_value: float) -> Callable[[float], float]:
    """
    Returns a function that computes
    `progress_remaining * initial_value`.
    """
    def func(progress_remaining: float) -> float:
        return max(progress_remaining * initial_value, 1e-4)  # Ensure a minimum value
    return func

namelist = ["ppo_256_256"]
checkpoint_name = namelist[0]+".zip"
reward_logger_name = namelist[0]+".csv"

# ---------- Entropy‑decay callback ---------------------------------------------

class EntropyDecayCallback(BaseCallback):
    """Linearly decays `model.ent_coef` from *start* → *end*.
    SB3 (≤2.0) stores `ent_coef` as a plain float, so scheduling must be manual.
    """
    def __init__(self, start: float, end: float, total_timesteps: int, verbose: int = 0):
        super().__init__(verbose)
        self.start = start
        self.end = end
        self.total = float(total_timesteps)

    def _on_step(self) -> bool:
        # progress_remaining: 1 → 0 over training
        progress_remaining = 1.0 - self.model.num_timesteps / self.total
        new_coef = self.end + (self.start - self.end) * progress_remaining
        self.model.ent_coef = new_coef
        return True

# ---------- MAIN TRAINING LOOP -------------------------------------------------

if __name__ == "__main__":
    # 0) RUN IDENTIFIER ---------------------------------------------------------
    TOTAL_TIMESTEPS = 6_000_000 # 10 million timesteps
    SAVE_DIR = "ppo_newreward"
    os.makedirs(SAVE_DIR, exist_ok=True)

    # 1) ENVIRONMENT ------------------------------------------------------------
    train_env = BipedEnv()
    # If you have a CurriculumWrapper defined, enable it like this:
    # train_env = CurriculumWrapper(train_env)

    # 2) CALLBACKS --------------------------------------------------------------
    checkpoint_cb = CustomCheckpointCallback(
        save_freq=500_000,
        save_path=SAVE_DIR,
        verbose=1,
    )
    reward_logger = RewardLoggerCallback(
        log_file=os.path.join(SAVE_DIR, "rewards.csv")
    )

    # Entropy decays linearly 1e‑3 → 0 across training
    ENT_START = 1e-3  # initial entropy coefficient
    ENT_END   = 3e-4  # final entropy coefficient
    entropy_decay_cb = EntropyDecayCallback(ENT_START, ENT_END, TOTAL_TIMESTEPS)

    callback_list = CallbackList([checkpoint_cb, reward_logger, entropy_decay_cb])

    # 3) MODEL CONFIGURATION ----------------------------------------------------
    policy_kwargs = dict(
        activation_fn=torch.nn.ReLU,
        net_arch=dict(pi=[256, 256], vf=[256, 256])    
        )

    model = PPO(
        policy="MlpPolicy",
        env=train_env,
        device="cpu",  # if you dont use cnn use cpu, if you use cnn use cuda
        tensorboard_log=SAVE_DIR,

        # # --- Core PPO hyper‑parameters ---------------------------------------
        n_steps=8192,
        batch_size=128,  # big minibatches for smoother advantages
        n_epochs=5,
        clip_range=0.18,  # 0.2
        # clip_range_vf=None,
        target_kl=0.2,  # hard KL ceiling

        # learning_rate=linear_schedule(3e-4),  # decay from 3e‑4 → 1e-4
        learning_rate=3e-4,  # decay from 3e‑4 → 1e-4
        ent_coef= ENT_START,          # no deduction constant scalar
        policy_kwargs=policy_kwargs
    )

    # 4) TRAIN -----------------------------------------------------------------

    model.learn(
        total_timesteps=TOTAL_TIMESTEPS,
        callback=callback_list,
    )

    # 5) SAVE FINAL ARTIFACTS --------------------------------------------------
    model.save(os.path.join(SAVE_DIR, "final_model"))
    print(f"Training complete. Models and logs are in: {SAVE_DIR}")
    print(f"Total training time: {time.time() - t0:.2f} seconds")       


import numpy as np
from scipy.signal import resample
import torch
from gait_generator_net import SimpleFCNN
import gymnasium as gym
from gymnasium import spaces
import pybullet as p
import pybullet_data
import time
from PIL import Image

class BipedEnv(gym.Env):
    def __init__(self,render=False, render_mode= None, demo_mode=False, demo_type=None):
        self.init_no = 0
        if render_mode == 'human':
            self.physics_client = p.connect(p.GUI)
        else:
            self.physics_client = p.connect(p.DIRECT)
        self.observe_mode = False
        self.scale = 1.
        self.dt = 1e-3
        self.demo_mode = demo_mode
        self.demo_type = demo_type
        p.setAdditionalSearchPath(pybullet_data.getDataPath())
        if self.demo_mode == True:
            p.setPhysicsEngineParameter(
                fixedTimeStep       = 1.0/1000.0,
                numSolverIterations = 100,
                deterministicOverlappingPairs = 1,
                enableConeFriction  = 0
            )           
        
        self.robot = p.loadURDF("assets/biped2d.urdf", [0,0,1.185], p.getQuaternionFromEuler([0.,0.,0.]),physicsClientId=self.physics_client)
        self.planeId = p.loadURDF("plane.urdf",physicsClientId=self.physics_client)
        self.leg_len = 0.94
        self.render_mode = render_mode
        self.joint_idx = [2,3,4,5,6,7,8]

        self.max_steps = int(3*(1/self.dt))
        self.action_space = spaces.Box(low=-1, high=1, shape=(7,), dtype=np.float32)
        self.observation_space = spaces.Box(low=-100, high=100, shape=(58,), dtype=np.float32)

        self.t = 0
        self.gaitgen_net = SimpleFCNN()
        self.gaitgen_net.load_state_dict(torch.load('final_model.pth',weights_only=True))
        
        self.normalizationconst = np.load(rf"newnormalization_constants.npy")
        self.joint_no = p.getNumJoints(self.robot)
        self.max_torque = np.array([500,500,300,150,500,300,150])  # max torque for each joint defined in urdf file
        self.state = np.zeros(58)
        self.update_const = 0.75
        self.velocity_norrmcoeff = 10.0
        self.pos_normcoeff = np.pi
        self.torque_normcoeff = 500

    def reset(self,seed=None,test_speed = None, test_angle = None,demo_max_steps = None, 
              ground_noise = None, ground_resolution = None,heightfield_data=None):
        self.test_speed = test_speed
        self.test_angle = test_angle
        self.max_steps = int(3*(1/self.dt))
        self.t = 0
        self.init_no += 1
        p.resetSimulation(physicsClientId=self.physics_client)
        self.reference_speed = 0.1 + np.random.rand()*2.5
        ramp_Limit = 6 #* (1 / (1+ np.exp(-self.init_no / 7000))) # sigmoid function to scale the ramp angle
        self.ramp_angle = np.random.uniform(-ramp_Limit,ramp_Limit) *np.pi / 180

        if self.demo_mode == True:
            if demo_max_steps:
                self.max_steps = demo_max_steps
            if self.test_speed is not None:
                self.reference_speed = self.test_speed

            if self.test_angle is not None:
                self.ramp_angle = self.test_angle *np.pi / 180

        encoder_vec = np.empty((3))   # init_pos + speed + r_leglength + l_leglength + ramp_angle = 0
        encoder_vec[0] = self.reference_speed/3
        encoder_vec[1] = self.leg_len /1.5
        encoder_vec[2] = self.leg_len /1.5
        encoder_vec = torch.tensor(encoder_vec, dtype=torch.float32)    
        self.reference = self.findgait(encoder_vec)                     #Find the gait
        self.reference = np.clip(self.reference, -np.pi/2, np.pi/2)     #Clip the gait

        plane_orientation = p.getQuaternionFromEuler([self.ramp_angle, 0 , 0])

        if self.demo_mode == False:
            self.planeId = p.loadURDF("plane.urdf",physicsClientId=self.physics_client, baseOrientation=plane_orientation)
            p.changeDynamics(self.planeId, -1, lateralFriction=1.0)
        else:
            if (heightfield_data.any() != None):
                self.init_noisy_plane(ground_resolution=ground_resolution, noise_level=ground_noise,baseOrientation=plane_orientation,
                                      heightfield_data=heightfield_data)
                self.heightfield_data = heightfield_data
            else:
                self.planeId = p.loadURDF("plane.urdf",physicsClientId=self.physics_client, baseOrientation=plane_orientation)
                p.changeDynamics(self.planeId, -1, lateralFriction=1.0)


        self.reset_info = {'current state':self.state}
        self.past_action_error = np.zeros(7)
        self.current_action = np.zeros(7)
        self.target_action = np.zeros(7)
        self.past_target_action = np.zeros(7)
        self.past2_target_action = np.zeros(7)
        self.past_forward_place = 0
        self.control_freq = 10
        self.external_states = np.zeros(4)
        self.ground_noise = ground_noise if ground_noise is not None else 0.0
        self.init_state()
        self.return_state()
        return self.state, self.reset_info

    def step(self,torques):
        # Set torques
        self.target_action = torques * self.max_torque
        for i in range(10):
            self.current_action = self.update_const*self.target_action + (1-self.update_const)*self.current_action 
            self.t +=1
            p.setJointMotorControlArray(
                bodyIndex=self.robot,
                jointIndices=self.joint_idx,
                controlMode=p.TORQUE_CONTROL,
                forces=self.current_action,
                physicsClientId=self.physics_client
            )
            # Step simulation
            p.stepSimulation()
            
        self.past_target_action = self.target_action
        self.past2_target_action = self.past_target_action
        self.return_state()

        if self.render_mode == 'human':
            time.sleep(self.dt)
        reward, done = self.biped_reward(self.state,torques=self.target_action)
        truncated = False

        if self.t > self.max_steps:
            truncated = True
        return self.state, reward, done, truncated, self.state_info

    def biped_reward(self,x,torques):
        self.imitation_weight_hip = 0.75
        self.imitation_weight_knee = 0.75
        self.imitation_weight_ankle = 0.25
        # 10 M steps is usually 35k episodes
        self.alive_weight = 0.5
        self.contact_weight = 0.5
        done = False
        reward = 0

        #Contact Reward
        contact_points = p.getContactPoints(self.robot, self.planeId)
        left_contact = len([i for i in contact_points if i[3] == 8])
        right_contact = len([i for i in contact_points if i[3] == 5])

        if left_contact >= 2 and right_contact == 0 :
            reward += self.contact_weight
        elif right_contact >= 2 and left_contact == 0:
            reward += self.contact_weight
        elif left_contact > 1 and right_contact > 1:
            reward += self.contact_weight


        #Imitation Reward
        hip_joint_pos = x[[7,10]] *self.pos_normcoeff
        hip_ref_pos = x[[34,37]] *self.pos_normcoeff
        reward += self.imitation_weight_hip * np.exp(-5*np.linalg.norm(hip_joint_pos - hip_ref_pos))

        knee_joint_pos = x[[8,11]] *self.pos_normcoeff
        knee_ref_pos = x[[35,38]] *self.pos_normcoeff
        reward += self.imitation_weight_knee *np.exp(-5*np.linalg.norm(knee_joint_pos - knee_ref_pos))

        ankle_joint_pos = x[[9,12]] *self.pos_normcoeff
        ankle_ref_pos = x[[36,39]] *self.pos_normcoeff
        reward += self.imitation_weight_ankle *np.exp(-5*np.linalg.norm(ankle_joint_pos - ankle_ref_pos))

        hip_joint_vel = x[[28,31]] * self.velocity_norrmcoeff
        hip_ref_vel = x[[52,55]] * self.velocity_norrmcoeff
        reward += self.imitation_weight_hip * 0.2 * np.exp(-0.1*np.linalg.norm(hip_joint_vel - hip_ref_vel))

        knee_joint_vel = x[[29,32]] * self.velocity_norrmcoeff
        knee_ref_vel = x[[53,56]] * self.velocity_norrmcoeff
        reward += self.imitation_weight_knee * 0.2 * np.exp(-0.1*np.linalg.norm(knee_joint_vel - knee_ref_vel))

        ankle_joint_vel = x[[30,33]] * self.velocity_norrmcoeff
        ankle_ref_vel = x[[54,57]] * self.velocity_norrmcoeff
        reward += self.imitation_weight_ankle * 0.2 * np.exp(-0.1*np.linalg.norm(ankle_joint_vel - ankle_ref_vel))

        #Torque Reward
        reward -= 1e-3 * np.mean(np.abs(self.target_action))
        
        # Forward Speed Reward
        current_speed = (self.external_states[1] - self.past_forward_place) / (self.dt * 10)  # forward speed
        reward += 0.5  * np.exp(-2*np.abs(current_speed - self.reference_speed))  # reward for maintaining speed newly adjusted

        #Angle Reward
        if np.abs(self.external_states[3]) > 0.98:  # Robot outside healthy angle range
            reward =- 50
            done = True

        if self.demo_type != "noisy":
            #Height Reward
            if self.external_states[2] > 1.45 + np.tan(self.ramp_angle) * self.external_states[1]:
                reward =- 50
                done = True
            elif self.external_states[2] > 1.3+ np.tan(self.ramp_angle) * self.external_states[1]:
                reward -= self.alive_weight
            elif self.external_states[2] < 0.8+ np.tan(self.ramp_angle) * self.external_states[1]:
                reward =- 50
                done = True
            elif self.external_states[2] < 0.98+ np.tan(self.ramp_angle) * self.external_states[1]:
                reward -= 1 * self.alive_weight
            else:
                reward += 1 * self.alive_weight
        
        else:
            x_pos = self.external_states[1]
            plane_z_location = self.heightfield_data[int((x_pos / 0.05+512)*32)]
            #Height Reward
            if self.external_states[2] > 1.45 + np.tan(self.ramp_angle) * self.external_states[1] + plane_z_location:
                reward =- 50
                done = True
            elif self.external_states[2] > 1.3+ np.tan(self.ramp_angle) * self.external_states[1] + plane_z_location:
                reward -= self.alive_weight
            elif self.external_states[2] < 0.8+ np.tan(self.ramp_angle) * self.external_states[1] + plane_z_location:
                reward =- 50
                done = True
            elif self.external_states[2] < 0.98+ np.tan(self.ramp_angle) * self.external_states[1] + plane_z_location:
                reward -= 1 * self.alive_weight
            else:
                reward += 1 * self.alive_weight
        return reward, done
    
    def close(self):
        self.physics_client.disconnect()
        print("Environment closed")


    def findgait(self,input_vec):

        freqs = self.gaitgen_net(input_vec)
        predictions = freqs.reshape(-1,6,2,17)
        predictions = predictions.detach().numpy()
        predictions = predictions[0]
        predictions = self.denormalize(predictions)
        pred_time = self.pred_ifft(predictions)

        return pred_time

    def denormalize(self,pred):
        #form is [5,2,17]
        for i in range(17):
            for k in range(2):
                pred[:,k,i] = pred[:,k,i] * self.normalizationconst[i*2+k]
        return pred
    
        
    def pred_ifft(self,predictions):
        #form is [5,2,17]
        real_pred = predictions[:,0,:]
        imag_pred = predictions[:,1,:]
        predictions = real_pred + 1j*imag_pred

        pred_time = np.fft.irfft(predictions, axis=1)
        pred_time = pred_time.transpose(1,0)
        org_rate = 10

        if self.dt < 0.1:
            num_samples = int((pred_time.shape[0]) * (1/self.dt)/(org_rate))  # resample with self.dt
            # Upsample using Fourier method
            pred_time = resample(pred_time, num_samples, axis=0)
            pred_time = np.tile(pred_time, (5,1))    # Create loop for reference movement
        return pred_time


    def starting_height(self,hip_init,knee_init,ankle_init):


        upper_len = 0.45
        lower_len = 0.45
        foot_len = 0.09

        hip_short = upper_len - (upper_len * np.cos(hip_init) )
        knee_short = lower_len - (lower_len * np.cos(knee_init))
        foot_exten = foot_len * np.sin(np.abs(ankle_init))
        init_pos = 1.195 - hip_short - knee_short 

        return init_pos
    

    def init_state(self):
        if self.demo_mode == False:

            start_idx = np.random.randint(0,500)

            # self.max_steps = self.max_steps - start_idx
            self.reference_idx = start_idx

            rhip_pos = self.reference[start_idx,0]
            rknee_pos = self.reference[start_idx,1]
            rankle_pos = self.reference[start_idx,2]
            lhip_pos = self.reference[start_idx,3]
            lknee_pos = self.reference[start_idx,4]
            lankle_pos = self.reference[start_idx,5]
            
            if np.abs(rhip_pos) > np.abs(lhip_pos):
                hip_init = lhip_pos
            else:
                hip_init = rhip_pos

            if np.abs(rknee_pos) > np.abs(lknee_pos):
                knee_init = lknee_pos
            else:
                knee_init = rknee_pos

            if np.abs(rankle_pos) < np.abs(lankle_pos):
                ankle_init = lankle_pos
            else:
                ankle_init = rankle_pos

            init_z = self.starting_height(hip_init,knee_init,ankle_init)
            del self.robot
            self.robot = p.loadURDF("assets/biped2d.urdf", [0,0,init_z + 0.02], p.getQuaternionFromEuler([0.,0.,0.]))
            p.setJointMotorControlArray(self.robot,[0,1,2,3,4,5,6,7,8], p.VELOCITY_CONTROL, forces=[0,0,0,0,0,0,0,0,0])

            p.resetJointState(self.robot, 3, targetValue = rhip_pos) 
            p.resetJointState(self.robot, 4, targetValue = rknee_pos)
            p.resetJointState(self.robot, 5, targetValue = rankle_pos)
            p.resetJointState(self.robot, 6, targetValue = lhip_pos)
            p.resetJointState(self.robot, 7, targetValue = lknee_pos)
            p.resetJointState(self.robot, 8, targetValue = lankle_pos)
        else:
            start_idx = 0
            self.reference_idx = start_idx

            rhip_pos = 0.0
            rknee_pos = 0.0
            rankle_pos = 0.0
            lhip_pos = 0.0
            lknee_pos = 0.0
            lankle_pos = 0.0
            

            init_z = self.starting_height(rhip_pos,lhip_pos,rankle_pos)
            del self.robot
            self.robot = p.loadURDF("assets/biped2d.urdf", [0,0,1.185], p.getQuaternionFromEuler([0.,0.,0.]))
            p.setJointMotorControlArray(self.robot,[0,1,2,3,4,5,6,7,8], p.VELOCITY_CONTROL, forces=[0,0,0,0,0,0,0,0,0])

            p.resetJointState(self.robot, 3, targetValue = rhip_pos) 
            p.resetJointState(self.robot, 4, targetValue = rknee_pos)
            p.resetJointState(self.robot, 5, targetValue = rankle_pos)
            p.resetJointState(self.robot, 6, targetValue = lhip_pos)
            p.resetJointState(self.robot, 7, targetValue = lknee_pos)
            p.resetJointState(self.robot, 8, targetValue = lankle_pos)

        p.setGravity(0,0,-9.81)
        p.setTimeStep(self.dt)

        self.t1_torso_pos = p.getJointState(self.robot, 2)[0]
        self.t1_rhip_pos = p.getJointState(self.robot, 3)[0]
        self.t1_rknee_pos = p.getJointState(self.robot, 4)[0]
        self.t1_rankle_pos = p.getJointState(self.robot, 5)[0]
        self.t1_lhip_pos = p.getJointState(self.robot, 6)[0]
        self.t1_lknee_pos = p.getJointState(self.robot, 7)[0]
        self.t1_lankle_pos = p.getJointState(self.robot, 8)[0]


    def return_state(self):
        
        link_state = p.getLinkState(self.robot, 2,computeLinkVelocity=True)          #link index 2 is for torso
        torso_g_quat = link_state[1]
        roll, _, _ = p.getEulerFromQuaternion(torso_g_quat)
        (pos_x,pos_y,pos_z) = link_state[0]                #3D position of the link
        y_vel = link_state[6][1]                           #y velocity of the link

        self.torso_pos = p.getJointState(self.robot, 2)[0]
        self.rhip_pos = p.getJointState(self.robot, 3)[0]
        self.rknee_pos = p.getJointState(self.robot, 4)[0]
        self.rankle_pos = p.getJointState(self.robot, 5)[0]
        self.lhip_pos = p.getJointState(self.robot, 6)[0]
        self.lknee_pos = p.getJointState(self.robot, 7)[0]
        self.lankle_pos = p.getJointState(self.robot, 8)[0]

        self.torso_vel = p.getJointState(self.robot, 2)[1]
        self.rhip_vel = p.getJointState(self.robot, 3)[1]
        self.rknee_vel = p.getJointState(self.robot, 4)[1]
        self.rankle_vel = p.getJointState(self.robot, 5)[1]
        self.lhip_vel = p.getJointState(self.robot, 6)[1]
        self.lknee_vel = p.getJointState(self.robot, 7)[1]
        self.lankle_vel = p.getJointState(self.robot, 8)[1]

        ref_rhip_vel = (self.reference[self.reference_idx+self.t,0] - self.reference[self.reference_idx+self.t-1,0])/self.dt
        ref_rknee_vel = (self.reference[self.reference_idx+self.t,1] - self.reference[self.reference_idx+self.t-1,1])/self.dt
        ref_rankle_vel = (self.reference[self.reference_idx+self.t,2] - self.reference[self.reference_idx+self.t-1,2])/self.dt
        ref_lhip_vel = (self.reference[self.reference_idx+self.t,3] - self.reference[self.reference_idx+self.t-1,3])/self.dt
        ref_lknee_vel = (self.reference[self.reference_idx+self.t,4] - self.reference[self.reference_idx+self.t-1,4])/self.dt
        ref_lankle_vel = (self.reference[self.reference_idx+self.t,5] - self.reference[self.reference_idx+self.t-1,5])/self.dt

        self.state[0] = self.reference_speed /3 
        self.state[1] = self.ramp_angle 
        self.state[2] = 0
        self.state[3] = 0
        self.state[4] = 0
        
        self.past_forward_place = self.external_states[1]
        self.external_states = [pos_x,pos_y,pos_z,roll]

        self.state[5] = y_vel   / 3

        self.state[6:13] = np.array([self.torso_pos, self.rhip_pos, self.rknee_pos, self.rankle_pos, self.lhip_pos, self.lknee_pos, self.lankle_pos]) /self.pos_normcoeff

        self.state[13:20] = np.array([self.past_target_action[0]/self.max_torque[0], self.past_target_action[1]/self.max_torque[1], self.past_target_action[2]/self.max_torque[2], 
                             self.past_target_action[3]/self.max_torque[3], self.past_target_action[4]/self.max_torque[4], self.past_target_action[5]/self.max_torque[5], 
                             self.past_target_action[6]/self.max_torque[6]])
        
        self.state[20:27] = np.array([self.t1_torso_pos, self.t1_rhip_pos, self.t1_rknee_pos, self.t1_rankle_pos, self.t1_lhip_pos, 
                             self.t1_lknee_pos, self.t1_lankle_pos]) /self.pos_normcoeff
        # self.state[27:34] = [self.past2_target_action[0]/self.max_torque, self.past2_target_action[1]/self.max_torque, self.past2_target_action[2]/self.max_torque,
        #                      self.past2_target_action[3]/self.max_torque, self.past2_target_action[4]/self.max_torque, self.past2_target_action[5]/self.max_torque,
        #                      self.past2_target_action[6]/self.max_torque]
        self.state[27:34] = np.array([self.torso_vel, self.rhip_vel, self.rknee_vel, self.rankle_vel, self.lhip_vel, 
                             self.lknee_vel, self.lankle_vel]) /self.velocity_norrmcoeff

        self.state[34:40] = np.array([self.reference[self.reference_idx+self.t,0], self.reference[self.reference_idx+self.t,1], 
                             self.reference[self.reference_idx+self.t,2], self.reference[self.reference_idx+self.t,3],
                             self.reference[self.reference_idx+self.t,4], self.reference[self.reference_idx+self.t,5]]) / self.pos_normcoeff
        
        self.state[40:46] = np.array([self.reference[self.reference_idx+self.t+10,0], self.reference[self.reference_idx+self.t+10,1],
                                self.reference[self.reference_idx+self.t+10,2], self.reference[self.reference_idx+self.t+10,3],
                                self.reference[self.reference_idx+self.t+10,4], self.reference[self.reference_idx+self.t+10,5]]) / self.pos_normcoeff
        
        # self.state[53:59] = [self.reference[self.reference_idx+self.t+50,0], self.reference[self.reference_idx+self.t+50,1],
        #                         self.reference[self.reference_idx+self.t+50,2], self.reference[self.reference_idx+self.t+50,3],
        #                         self.reference[self.reference_idx+self.t+50,4], self.reference[self.reference_idx+self.t+50,5]]
        
        self.state[46:52] = np.array([self.reference[self.reference_idx+self.t+100,0], self.reference[self.reference_idx+self.t+100,1],
                                self.reference[self.reference_idx+self.t+100,2], self.reference[self.reference_idx+self.t+100,3],
                                self.reference[self.reference_idx+self.t+100,4], self.reference[self.reference_idx+self.t+100,5]]) / self.pos_normcoeff
        
        self.state[52:58] = np.array([ref_rhip_vel, ref_rknee_vel, ref_rankle_vel,ref_lhip_vel, ref_lknee_vel, ref_lankle_vel]) /self.velocity_norrmcoeff
        
        self.t1_torso_pos = self.torso_pos
        self.t1_rhip_pos = self.rhip_pos
        self.t1_rknee_pos = self.rknee_pos
        self.t1_rankle_pos = self.rankle_pos
        self.t1_lhip_pos = self.lhip_pos
        self.t1_lknee_pos = self.lknee_pos
        self.t1_lankle_pos = self.lankle_pos

        self.state_info = {0:"reference_speed",
                    1:"ramp_angle", 2:"pos_x (external)", 3:"pos_y (external)", 4:"pos_z (external)", 5:"y_vel",

                    6:"torso_pos", 7:"rhip_pos", 8:"rknee_pos", 9:"rankle_pos", 10:"lhip_pos", 11:"lknee_pos", 12:"lankle_pos",
                    #self.past_target_action
                    13:"t1_torso_action", 14:"t1_rhip_action", 15:"t1_rknee_action", 16:"t1_rankle_action", 17:"t1_lhip_action", 18:"t1_lknee_action", 19:"t1_lankle_action",

                    20:"t1torso_pos", 21:"t1rhip_pos", 22:"t1rknee_pos", 23:"t1rankle_pos", 24:"t1lhip_pos", 25:"t1lknee_pos", 26:"t1lankle_pos",
                    #self.past2_target_action
                    #27:"t2_torso_action", 28:"t2_rhip_action", 29:"t2_rknee_action", 30:"t2_rankle_action", 31:"t2_lhip_action", 32:"t2_lknee_action", 33:"t2_lankle_action",

                    27:"torso_vel", 28:"rhip_vel", 29:"rknee_vel", 30:"rankle_vel", 31:"lhip_vel", 32:"lknee_vel", 33:"lankle_vel",
                    
                    34:"t1_ref_rhip", 35:"t1_ref_rknee",36:"t1_ref_rankle" ,37:"t1_ref_lhip", 38:"t1_ref_lknee",39:"t1_ref_lankle",

                    40:"t2_ref_rhip", 41:"t2_ref_rknee",42:"t2_ref_rankle", 43:"t2_ref_lhip", 44:"t2_ref_lknee",45:"t2_ref_lankle",

                    #53:"t50_ref_rhip", 54:"t50_ref_rknee",55:"t50_ref_rankle", 56:"t50_ref_lhip", 57:"t50_ref_lknee",58:"t50_ref_lankle",

                    46:"t100_ref_rhip", 47:"t100_ref_rknee",48:"t100_ref_rankle", 49:"t100_ref_lhip", 50:"t100_ref_lknee",51:"t100_ref_lankle",

                    52:'ref_rhip_vel', 53:'ref_rknee_vel',54:"ref_rankle_vel", 55:'ref_lhip_vel', 56:'ref_lknee_vel',57:"ref_lankle_vel"}

        # return self.state, self.state_info
    

    def return_external_state(self):
        return self.external_states

    def init_noisy_plane(self, noise_level=0.1,ground_resolution= 0.05 ,num_rows=32, num_columns=1024,
                        baseOrientation=None,heightfield_data=None):

        # Use identity quaternion if none provided.
        mesh_scale=[ground_resolution, ground_resolution, 1]
        if baseOrientation is None:
            baseOrientation = p.getQuaternionFromEuler([0, 0, 0])

        # Create a collision shape for the heightfield
        terrain_shape = p.createCollisionShape(
            shapeType=p.GEOM_HEIGHTFIELD,
            meshScale=mesh_scale,
            heightfieldData=heightfield_data,
            numHeightfieldRows=num_rows,
            numHeightfieldColumns=num_columns,
            physicsClientId=self.physics_client
        )
        min_height = np.min(heightfield_data)
        max_height = np.max(heightfield_data)
        z_center = 0.5 * (min_height + max_height) * mesh_scale[2]
        # Create a static (mass=0) multi-body using the heightfield shape
        self.planeId = p.createMultiBody(
            baseMass=0,
            baseCollisionShapeIndex=terrain_shape,
            basePosition=[0, 0, z_center],
            baseOrientation=baseOrientation,
            physicsClientId=self.physics_client
        )

        p.changeDynamics(self.planeId, -1, lateralFriction=1.0)

    def get_image(self):
        view_matrix = p.computeViewMatrix(
            cameraEyePosition=[3, 0, 1.5],  # farther and higher
            cameraTargetPosition=[0, 0, 1.0],
            cameraUpVector=[0, 0, 1]
        )

        # Use square aspect and wide FOV
        projection_matrix = p.computeProjectionMatrixFOV(
            fov=75,               # wider field of view
            aspect=1.0,           # square image
            nearVal=0.1,
            farVal=100.0
        )

        # Capture square image
        res = 640
        _, _, rgbImg, _, _ = p.getCameraImage(
            width=res,
            height=res,
            viewMatrix=view_matrix,
            projectionMatrix=projection_matrix
        )

        # Convert and save image
        rgb_array = np.reshape(rgbImg, (res, res, 4))
        image = Image.fromarray(rgb_array[:, :, :3], 'RGB')
        return image