{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pybullet_bipedenv_torquecontrolled import BipedEnv\n",
    "from pybullet_bipedenv_poscontrolled import POS_Biped\n",
    "import os\n",
    "import numpy as np\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CallbackList\n",
    "from stable_baselines3 import PPO, SAC\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class RewardLoggerCallback(BaseCallback):\n",
    "    def __init__(self, log_file: str, verbose: int = 0):\n",
    "        super(RewardLoggerCallback, self).__init__(verbose)\n",
    "        self.log_file = log_file\n",
    "        self.episode_rewards = []\n",
    "        self.current_episode_reward = 0\n",
    "        self.current_step = 0  # Track the number of steps in the current episode\n",
    "\n",
    "        # Create the log file if it doesn't exist\n",
    "        if not os.path.exists(self.log_file):\n",
    "            with open(self.log_file, 'w') as f:\n",
    "                f.write(\"Episode,Total Reward,Termination Step\\n\")\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Check if the episode has ended\n",
    "        dones = self.locals[\"dones\"]\n",
    "        rewards = self.locals[\"rewards\"]\n",
    "\n",
    "        # Accumulate rewards for the current episode\n",
    "        self.current_episode_reward += rewards[0]\n",
    "        self.current_step += 1  # Increment step count for the current episode\n",
    "\n",
    "        # If the episode is done, log the reward and termination step\n",
    "        if dones[0]:\n",
    "            self.episode_rewards.append(self.current_episode_reward)\n",
    "            with open(self.log_file, 'a') as f:\n",
    "                f.write(f\"{len(self.episode_rewards)},{self.current_episode_reward},{self.current_step}\\n\")\n",
    "            \n",
    "            # Reset counters for the next episode\n",
    "            self.current_episode_reward = 0\n",
    "            self.current_step = 0\n",
    "\n",
    "        return True\n",
    "    def _on_training_end(self) -> None:\n",
    "        # Optionally summarize results at the end of training\n",
    "        print(\"Training finished. Total episodes:\", len(self.episode_rewards))\n",
    "        print(\"Episode rewards:\", self.episode_rewards)\n",
    "\n",
    "class CustomCheckpointCallback(BaseCallback):\n",
    "    def __init__(self, save_freq, save_path,init_no = 0, verbose=0):\n",
    "        super(CustomCheckpointCallback, self).__init__(verbose)\n",
    "        self.save_freq = save_freq\n",
    "        self.save_path = save_path\n",
    "        self.init_no = init_no\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        done = False\n",
    "        # Save the model every `save_freq` steps\n",
    "        if self.n_calls % self.save_freq == 0:\n",
    "            self.init_no +=1\n",
    "            model_path = f\"weights/model_checkpoint_{self.init_no}\"+checkpoint_name\n",
    "            self.model.save(model_path)\n",
    "            if self.verbose > 0:\n",
    "                print(f\"Model saved at step {self.n_calls} to {model_path}\")\n",
    "                print(f\"Time taken for this checkpoint: {time.time() - t0:.2f} seconds\")\n",
    "                time.sleep(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "total_timesteps = 60000000\n",
    "t0 = time.time()\n",
    "namelist = [\"ppo_vf128_update\"]\n",
    "for i in range(len(namelist)):\n",
    "    rewar_Logger_name = namelist[i]+\".csv\"\n",
    "    checkpoint_name = namelist[i]+\".zip\"\n",
    "    weight_file_name = \"final_\"+namelist[i]\n",
    "    use_past_weights = False\n",
    "    past_weight_path = \"/home/baran/Bipedal-imitation-rl/weights/model_checkpoint_8ppo_vf128_update.zip\"\n",
    "    init_no = 100\n",
    "\n",
    "    if use_past_weights:\n",
    "        checkpoint_callback = CustomCheckpointCallback(\n",
    "            save_freq=500000, save_path='./checkpoints/',init_no=init_no, verbose=1\n",
    "        )\n",
    "    else:\n",
    "        checkpoint_callback = CustomCheckpointCallback(\n",
    "            save_freq=1000000, save_path='./checkpoints/', verbose=1\n",
    "        )\n",
    "    reward_logger = RewardLoggerCallback(log_file=rewar_Logger_name)\n",
    "\n",
    "    callbacks = CallbackList([checkpoint_callback, reward_logger])\n",
    "\n",
    "    env = BipedEnv(render_mode=None)\n",
    "\n",
    "    policy_kwargs = dict(net_arch=dict(pi=[128, 128], vf=[128, 128]))\n",
    "    print(\"Starting training\")\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        device=\"cpu\"\n",
    "        # learning_rate=1e-4,\n",
    "        # gamma=0.999,\n",
    "        # clip_range=0.1,\n",
    "        # ent_coef=0.01,\n",
    "        # vf_coef=0.75,\n",
    "        # batch_size=16,\n",
    "        # max_grad_norm=0.3,\n",
    "        # gae_lambda=0.99,\n",
    "        # verbose=0,\n",
    "    )\n",
    "    if use_past_weights:\n",
    "        model = PPO.load(past_weight_path,device=\"cpu\",env=env)\n",
    "        print(\"Loaded past weights\")\n",
    "\n",
    "    model.learn(total_timesteps=total_timesteps, callback=callbacks)\n",
    "\n",
    "    model.save(weight_file_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
