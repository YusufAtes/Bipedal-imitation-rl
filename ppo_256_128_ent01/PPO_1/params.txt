
policy_kwargs = dict(net_arch=dict(pi=[256, 128], vf=[256, 128]))
self.update_const = 0.45
self.control_freq = 10


def biped_reward(self,x,torques):

        self.alive_weight = 0.1
        self.forward_weight = 0.1
        self.ankle_weight = 0.1
        done = False
        reward = 0
        contact_points = self.p.getContactPoints(self.robot, self.planeId)
        # Conditions for early termination regarding stability

        if not contact_points:
            reward -=1  * self.alive_weight
        elif len(contact_points) > 2:
            reward +=1   * self.alive_weight
        else:
            reward -=1  * self.alive_weight
        
        if x[3] < 0:
            reward -= 1 * self.forward_weight

        if (x[7] > 0.15) and (x[10] > 0.15):
            reward -=1  * self.alive_weight
        elif (x[7] < -0.15) and (x[10] < -0.15):
            reward -=1  * self.alive_weight
        else:
            reward += 1 * self.alive_weight

        if x[4] > 1.5:
            reward -=1
            done = True
        elif x[4] > 1.2:
            reward -= self.alive_weight
        elif x[4] < 0.7:
            reward -=1
            done = True
        elif x[4] < 1.0:
            reward -= 1 * self.alive_weight
        else:
            reward += 1 * self.alive_weight

        joint_pos = x[[7,8,9,10,11,12]]
        ref_pos = x[[41,42,43,44,45,46]]
        reward += 1.25*np.exp(-2*np.linalg.norm(joint_pos - ref_pos)) 

        joint_vel = x[[35,36,37,38,39,40]]
        ref_vel = x[[52,53,54,55,56,57]]
        reward += 0.25* np.exp(-0.1*np.linalg.norm(joint_vel - ref_vel))

        reward -= 1e-3 * np.mean(np.abs(torques))
        reward += x[3] * 1e-1

        reward += 1.25*np.exp(-5*np.linalg.norm(x[5] - x[0]))

        return reward, done